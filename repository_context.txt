"""Package init."""
import json
from pathlib import Path

import streamlit as st

st.set_page_config(page_title="Edge CCTV Dashboard", layout="wide")
st.title("Edge CCTV Analytics")

col1, col2 = st.columns(2)
with col1:
    st.subheader("Recent Events")
    path = Path("artifacts/normalized/events.log")
    if path.exists():
        events = [json.loads(line) for line in path.read_text(encoding="utf-8").splitlines() if line]
        for event in events[-20:]:
            st.json(event)
    else:
        st.info("No events yet")

with col2:
    st.subheader("Placeholders")
    placeholders = Path("docs/placeholders.md").read_text(encoding="utf-8")
    st.markdown(placeholders)
from __future__ import annotations

import json
from pathlib import Path

from fastapi import FastAPI
from fastapi.responses import JSONResponse

app = FastAPI(title="Edge CCTV UI API")


def read_events(limit: int = 20) -> list[dict]:
    path = Path("artifacts/normalized/events.log")
    if not path.exists():
        return []
    lines = path.read_text(encoding="utf-8").strip().splitlines()
    events = [json.loads(line) for line in lines if line]
    return events[-limit:]


@app.get("/healthz")
def health() -> dict:
    return {"status": "ok"}


@app.get("/readyz")
def ready() -> dict:
    return {"ready": True}


@app.get("/events")
def events(limit: int = 20) -> JSONResponse:
    return JSONResponse(read_events(limit))


@app.get("/config/placeholders")
def placeholders() -> JSONResponse:
    table = Path("docs/placeholders.md").read_text(encoding="utf-8")
    return JSONResponse({"markdown": table})


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8080)
from __future__ import annotations

import base64
import hashlib
import json
import os
from pathlib import Path
from typing import Dict, Optional

from cryptography.fernet import Fernet


class EmbeddingStore:
    def __init__(self, path: Path, key_env: str = "SEC_EXPORT_ENCRYPTION_KEY") -> None:
        self.path = path
        self.path.parent.mkdir(parents=True, exist_ok=True)
        key_hex = os.getenv(key_env, "0" * 64)
        key_hex = (key_hex + "0" * 64)[:64]
        key_bytes = bytes.fromhex(key_hex)
        self.fernet = Fernet(base64.urlsafe_b64encode(key_bytes))
        if not self.path.exists():
            self.path.write_text(json.dumps({}), encoding="utf-8")

    def _load(self) -> Dict[str, str]:
        data = json.loads(self.path.read_text(encoding="utf-8"))
        return data

    def _save(self, data: Dict[str, str]) -> None:
        self.path.write_text(json.dumps(data, indent=2), encoding="utf-8")

    def enroll(self, name: str, embedding: str) -> None:
        data = self._load()
        encrypted = self.fernet.encrypt(embedding.encode("utf-8")).decode("utf-8")
        data[name] = encrypted
        self._save(data)

    def match(self, embedding: str, threshold: float) -> Optional[Dict[str, float]]:
        data = self._load()
        target = hashlib.sha256(embedding.encode("utf-8")).hexdigest()
        for name, encrypted in data.items():
            decrypted = self.fernet.decrypt(encrypted.encode("utf-8")).decode("utf-8")
            candidate_hash = hashlib.sha256(decrypted.encode("utf-8")).hexdigest()
            score = _cosine_like(target, candidate_hash)
            if score >= threshold:
                return {"identity": name, "score": score}
        return None


def _cosine_like(a_hex: str, b_hex: str) -> float:
    matches = sum(1 for a, b in zip(a_hex, b_hex) if a == b)
    return matches / len(a_hex)


__all__ = ["EmbeddingStore"]
"""Package init."""
from __future__ import annotations

import asyncio
import json
import logging
import time
from pathlib import Path

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from common.event_bus import Event, FileEventBus
from .store import EmbeddingStore
from common.config import load_yaml

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("privacy")

app = FastAPI(title="FRS Service")
store = EmbeddingStore(Path("artifacts/privacy/embeddings.json"))
audit_path = Path("artifacts/privacy/frs_audit.log")
audit_path.parent.mkdir(parents=True, exist_ok=True)
tracker_bus = FileEventBus(Path("artifacts/tracker"))
frs_config = load_yaml("configs/frs/config.yaml")
try:
    threshold = float(str(frs_config.get("threshold", "0.47")).strip("{}"))
except ValueError:
    threshold = 0.47

whitelist = load_yaml("configs/frs/whitelist.yaml") or {}
for person in whitelist.get("people", []):
    store.enroll(person.get("name", "unknown"), person.get("embedding", ""))


class EnrollRequest(BaseModel):
    name: str
    embedding: str
    operator: str
    reason: str


@app.post("/enroll")
def enroll(request: EnrollRequest) -> dict:
    if not request.name:
        raise HTTPException(status_code=400, detail="Name required")
    store.enroll(request.name, request.embedding)
    with audit_path.open("a", encoding="utf-8") as fh:
        fh.write(json.dumps({
            "ts": time.time(),
            "name": request.name,
            "operator": request.operator,
            "reason": request.reason,
        }) + "\n")
    return {"status": "ok"}


async def frs_worker() -> None:
    source = Path("artifacts/detections/events.log")
    offset = 0
    while True:
        if not source.exists():
            await asyncio.sleep(1)
            continue
        with source.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            for line in fh:
                offset = fh.tell()
                if not line.strip():
                    continue
                event = json.loads(line)
                if event.get("type") != "embedding":
                    continue
                embedding = event.get("embedding")
                match = store.match(embedding, threshold)
                if match:
                    tracker_bus.publish(
                        Event(
                            type="frs",
                            payload={
                                "camera_id": event.get("camera_id"),
                                "identity": match["identity"],
                                "score": match["score"],
                            },
                        )
                    )
        await asyncio.sleep(0.5)


@app.on_event("startup")
async def startup_event() -> None:
    asyncio.create_task(frs_worker())


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8081)
"""
Top-level package marker so modules under ``src`` can be imported as
``src.<module>`` (required for pytest and ``python -m`` entrypoints).
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List


@dataclass
class Benchmark:
    task: str
    model: str
    latency_ms: float
    accuracy: float
    vram_gb: float


def aggregate(records: List[dict]) -> List[Benchmark]:
    return [
        Benchmark(
            task=rec["task"],
            model=rec["model"],
            latency_ms=float(rec.get("latency_ms", 0.0)),
            accuracy=float(rec.get("accuracy", 0.0)),
            vram_gb=float(rec.get("vram_gb", 1.0)),
        )
        for rec in records
    ]
"""Autonomous maintainer agent."""
"""Package init."""
from __future__ import annotations

import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

ROOT = Path(__file__).resolve().parents[3]
README = ROOT / "README.md"
MODELS = ROOT / "docs" / "MODELS.md"
DATASETS = ROOT / "docs" / "datasets.md"
PLACEHOLDERS = ROOT / "docs" / "placeholders.md"

PATTERN = re.compile(r"<!-- auto:start name=(?P<name>[a-zA-Z0-9_-]+) -->(?P<body>.*?)<!-- auto:end -->", re.S)
PLACEHOLDER_PATTERN = re.compile(r"{{([A-Z0-9_]+)}}")
PLACEHOLDER_COLUMNS = [
    "Placeholder",
    "File path(s)",
    "Purpose",
    "Required format / regex",
    "Example",
    "Security level",
    "Who provides",
    "When required",
    "Default",
    "Resolution status",
    "Last updated",
]
ALLOWED_PREFIXES = {
    "CAMERA_",
    "DATASET_",
    "FRS_",
    "MQTT_",
    "UI_",
    "ORG_",
    "STORAGE_",
    "SEC_",
    "METRICS_",
    "EVENTS_",
    "RETENTION_",
    "TIME_SYNC_",
    "INSTALL_",
    "GIT_",
}
EXEMPT_PREFIXES = {"PROMETHEUS_", "HANDOFF_"}


def _allowed_placeholder(name: str) -> bool:
    return any(name.startswith(prefix) for prefix in ALLOWED_PREFIXES | EXEMPT_PREFIXES)


def _extract_section(path: Path, name: str) -> str:
    text = path.read_text(encoding="utf-8")
    match = PATTERN.search(text)
    if not match:
        return ""
    # There may be multiple sections; iterate all matches
    for m in PATTERN.finditer(text):
        if m.group("name") == name:
            return m.group("body").strip()
    return ""


def update_section(path: Path, name: str, content: str) -> None:
    text = path.read_text(encoding="utf-8")
    replacement = f"<!-- auto:start name={name} -->\n{content}\n<!-- auto:end -->"
    new_text, count = PATTERN.subn(lambda m: replacement if m.group("name") == name else m.group(0), text)
    if count == 0:
        raise ValueError(f"Auto section '{name}' not found in {path}")
    path.write_text(new_text, encoding="utf-8")


def _render_recommended_models(models: List[Dict[str, Any]], limit: int = 6) -> str:
    header = "| Rank | Task | Model | FP16 (ms) | INT8 (ms) | Accuracy | License | Last Checked |"
    divider = "|------|------|-------|-----------|-----------|----------|---------|--------------|"
    rows = [header, divider]
    for rec in models[:limit]:
        rows.append(
            "| {rank} | {task} | {model} | {fp16:.2f} | {int8:.2f} | {acc:.3f} | {license} | {checked} |".format(
                rank=rec.get("rank", "-"),
                task=rec.get("task", "-"),
                model=rec.get("model", "-"),
                fp16=rec.get("latency_fp16_ms", rec.get("latency_ms", 0.0)),
                int8=rec.get("latency_int8_ms", rec.get("latency_ms", 0.0)),
                acc=rec.get("accuracy", 0.0),
                license=rec.get("license", "-"),
                checked=rec.get("last_checked", "-"),
            )
        )
    return "\n".join(rows)


def _render_models_matrix(models: List[Dict[str, Any]]) -> str:
    header = (
        "| Task | Model | Backbone/Size | Training Data | Benchmark | Latency (FP16/INT8 ms) | VRAM (GB) | Params (M) | License | Why pick | Links |"
    )
    divider = "|------|-------|----------------|---------------|-----------|------------------------|-----------|------------|---------|----------|-------|"
    rows = [header, divider]
    for rec in models:
        links = rec.get("links", [])
        link_str = "<br>".join(links)
        rows.append(
            "| {task} | {model} | {backbone} | {train} | {benchmark} | {lat_fp16:.2f}/{lat_int8:.2f} | {vram:.2f} | {params:.2f} | {license} | {why} | {links} |".format(
                task=rec.get("task", "-"),
                model=rec.get("model", "-"),
                backbone=rec.get("backbone", "-"),
                train=rec.get("training_data", "-"),
                benchmark=rec.get("benchmark", "-"),
                lat_fp16=rec.get("latency_fp16_ms", 0.0),
                lat_int8=rec.get("latency_int8_ms", rec.get("latency_fp16_ms", 0.0)),
                vram=rec.get("vram_gb", 0.0),
                params=rec.get("params_m", 0.0),
                license=rec.get("license", "-"),
                why=rec.get("why", "-"),
                links=link_str or "-",
            )
        )
    return "\n".join(rows)


def _render_datasets(datasets: List[Dict[str, Any]]) -> str:
    header = (
        "| Use Case | Bucket | Dataset | License | Size | Modality | Indoor/Outdoor | Pros | Cons | Fit | Bias Notes |"
    )
    divider = "|----------|--------|---------|---------|------|---------|----------------|------|------|-----|------------|"
    rows = [header, divider]
    for item in datasets:
        name = item.get("name", "-")
        link = item.get("link")
        dataset_cell = f"[{name}]({link})" if link else name
        rows.append(
            "| {use_case} | {bucket} | {dataset} | {license} | {size} | {modality} | {io} | {pros} | {cons} | {fit} | {bias} |".format(
                use_case=item.get("use_case", "-"),
                bucket=item.get("bucket", "-"),
                dataset=dataset_cell,
                license=item.get("license", "-"),
                size=item.get("size", "-"),
                modality=item.get("modality", "-"),
                io=item.get("indoor_outdoor", "-"),
                pros=item.get("pros", "-"),
                cons=item.get("cons", "-"),
                fit=item.get("fit", "-"),
                bias=item.get("bias_notes", "-"),
            )
        )
    return "\n".join(rows)


def _render_performance_tuning(tuning: List[Dict[str, Any]]) -> str:
    if not tuning:
        return "- No automated tuning recommendations captured in the latest run."
    lines = []
    for item in tuning:
        topic = item.get("topic", "Tuning")
        recommendation = item.get("recommendation", "")
        impact = item.get("impact", "")
        lines.append(f"- **{topic}**: {recommendation} (_Impact_: {impact})")
    return "\n".join(lines)


def _render_known_issues(issues: List[Dict[str, Any]]) -> str:
    if not issues:
        return "- No outstanding issues recorded by the agent."
    lines = []
    for issue in issues:
        title = issue.get("title", "Issue")
        impact = issue.get("impact", "")
        workaround = issue.get("workaround", "")
        severity = issue.get("severity", "info").upper()
        link = issue.get("link")
        description = issue.get("description", "").rstrip()
        impact_text = issue.get("impact", "").rstrip()
        workaround = issue.get("workaround", "").rstrip()
        body = f"**[{severity}] {title}** — {description.rstrip('.')}"
        if description:
            body += "."
        if impact_text:
            body += f" _Impact_: {impact_text.rstrip('.')}."
        if workaround:
            body += f" _Workaround_: {workaround.rstrip('.')}."
        if link:
            body += f" [Details]({link})"
        lines.append(f"- {body}")
    return "\n".join(lines)


def _render_changelog(run_date: datetime, models_count: int, datasets_count: int, placeholders_count: int) -> str:
    previous = _extract_section(README, "changelog")
    entries = [line.strip() for line in previous.splitlines() if line.strip()]
    new_entry = (
        f"- {run_date.strftime('%Y-%m-%d')}: Catalog refresh (models={models_count}, datasets={datasets_count}, placeholders={placeholders_count})."
    )
    entries = [new_entry] + [entry for entry in entries if entry != new_entry]
    return "\n".join(entries[:10])  # keep latest 10 entries


def _load_placeholder_metadata() -> Dict[str, Dict[str, str]]:
    table = _extract_section(PLACEHOLDERS, "placeholder-index")
    metadata: Dict[str, Dict[str, str]] = {}
    if not table:
        return metadata
    lines = [line for line in table.splitlines() if line.startswith("|")]
    for line in lines[2:]:  # skip header/divider
        parts = [part.strip() for part in line.strip().strip("|").split("|")]
        if len(parts) != len(PLACEHOLDER_COLUMNS):
            continue
        entry = dict(zip(PLACEHOLDER_COLUMNS, parts))
        metadata[entry["Placeholder"]] = entry
    return metadata


def _scan_placeholder_usage(root: Path) -> Dict[str, List[str]]:
    usage: Dict[str, set[str]] = {}
    skip_dirs = {".git", "artifacts", "__pycache__", "engines", "weights"}
    for path in root.rglob("*"):
        if path.is_dir():
            if path.name in skip_dirs:
                continue
            if any(part in skip_dirs for part in path.parts):
                continue
            continue
        if path == PLACEHOLDERS:
            continue
        if path.suffix in {".png", ".jpg", ".jpeg", ".mp4", ".onnx", ".engine"}:
            continue
        if any(part in skip_dirs for part in path.parts):
            continue
        try:
            text = path.read_text(encoding="utf-8")
        except (UnicodeDecodeError, OSError):
            continue
        for match in PLACEHOLDER_PATTERN.finditer(text):
            raw_name = match.group(1)
            if not _allowed_placeholder(raw_name):
                continue
            placeholder = f"{{{{{raw_name}}}}}"
            usage.setdefault(placeholder, set()).add(str(path.relative_to(ROOT)))
    return {key: sorted(values) for key, values in usage.items()}


def _render_placeholder_table(run_date: datetime) -> Dict[str, Any]:
    metadata = _load_placeholder_metadata()
    usage = _scan_placeholder_usage(ROOT)
    for placeholder, paths in usage.items():
        entry = metadata.setdefault(
            placeholder,
            {
                "Placeholder": placeholder,
                "File path(s)": "",
                "Purpose": "TODO - document",
                "Required format / regex": ".+",
                "Example": "",
                "Security level": "public",
                "Who provides": "TODO",
                "When required": "setup",
                "Default": "none",
                "Resolution status": "unknown",
                "Last updated": run_date.strftime("%Y-%m-%d"),
            },
        )
        entry["File path(s)"] = "<br>".join(paths)
        entry["Last updated"] = run_date.strftime("%Y-%m-%d")
        if not entry.get("Resolution status"):
            entry["Resolution status"] = "unknown"
    rows = [
        "| " + " | ".join(PLACEHOLDER_COLUMNS) + " |",
        "|" + "-------------|" * len(PLACEHOLDER_COLUMNS),
    ]
    filtered_keys = [key for key in metadata.keys() if _allowed_placeholder(key.strip("{}"))]
    for key in sorted(filtered_keys):
        entry = metadata[key]
        row = "| " + " | ".join(entry.get(col, "") or "-" for col in PLACEHOLDER_COLUMNS) + " |"
        rows.append(row)
    return {"table": "\n".join(rows), "count": len(filtered_keys)}


def update_docs(
    *,
    models: List[Dict[str, Any]],
    datasets: List[Dict[str, Any]],
    issues: List[Dict[str, Any]],
    tuning: List[Dict[str, Any]],
    metadata: Dict[str, Any],
    run_timestamp: datetime,
    apply: bool = True,
) -> Dict[str, Any]:
    recommended = _render_recommended_models(models)
    matrix = _render_models_matrix(models)
    dataset_table = _render_datasets(datasets)
    tuning_md = _render_performance_tuning(tuning)
    issues_md = _render_known_issues(issues)
    placeholder_payload = _render_placeholder_table(run_timestamp)
    changelog_md = _render_changelog(run_timestamp, len(models), len(datasets), placeholder_payload["count"])

    if apply:
        update_section(README, "recommended-models", recommended)
        update_section(README, "performance-tuning", tuning_md)
        update_section(README, "known-issues", issues_md)
        update_section(README, "changelog", changelog_md)
        update_section(MODELS, "models-matrix", matrix)
        update_section(DATASETS, "datasets", dataset_table)
        update_section(PLACEHOLDERS, "placeholder-index", placeholder_payload["table"])

    return {
        "placeholders_count": placeholder_payload["count"],
        "models_matrix_rows": len(models),
        "datasets_rows": len(datasets),
        "tuning_count": len(tuning),
        "issues_count": len(issues),
    }
"""Package init."""
from __future__ import annotations

import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict

LOGGER = logging.getLogger("agent.scan")


class ModelScanner:
    """Loads curated catalog data and prepares it for downstream ranking.

    In online mode this class can be extended to fetch remote benchmarks or
    scrape vendor release notes. The offline catalog keeps the automation
    reproductible inside CI while awaiting network approval.
    """

    def __init__(self, catalog_path: Path | None = None) -> None:
        self.catalog_path = catalog_path or Path(__file__).resolve().parent.parent / "data" / "catalog.json"

    def scan(self) -> Dict[str, Any]:
        if not self.catalog_path.exists():
            raise FileNotFoundError(f"Catalog file not found: {self.catalog_path}")
        LOGGER.info("Loading catalog data from %s", self.catalog_path)
        raw = json.loads(self.catalog_path.read_text(encoding="utf-8"))
        raw["scan_timestamp"] = datetime.now(timezone.utc).isoformat()
        return raw
from __future__ import annotations

import argparse
import datetime as dt
import json
from pathlib import Path
from typing import Any, Dict

try:
    from .webscan.scan import ModelScanner  # type: ignore
    from .rank.rank import rank_candidates  # type: ignore
    from .writer.apply import update_docs  # type: ignore
except ImportError:  # pragma: no cover - support script execution
    import sys

    CURRENT_DIR = Path(__file__).resolve().parent
    PACKAGE_ROOT = CURRENT_DIR.parent
    if str(PACKAGE_ROOT) not in sys.path:
        sys.path.append(str(PACKAGE_ROOT))
    from webscan.scan import ModelScanner  # type: ignore
    from rank.rank import rank_candidates  # type: ignore
    from writer.apply import update_docs  # type: ignore

ROOT = Path(__file__).resolve().parent.parent.parent
HANDOFF = ROOT / "HANDOFF.md"


def write_json(path: Path, payload: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def write_pr_body(path: Path, summary: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    lines = [
        "## Change Summary",
        "| Item | Count |",
        "|------|-------|",
        f"| Models catalogued | {summary['models_count']} |",
        f"| Datasets catalogued | {summary['datasets_count']} |",
        f"| Placeholder entries | {summary['placeholders_count']} |",
        f"| Issues tracked | {summary['issues_count']} |",
        "",
        "## Sources",
    ]
    if summary.get("sources"):
        lines.extend([f"- {src}" for src in summary["sources"]])
    else:
        lines.append("- (no external sources listed)")
    lines.extend(
        [
            "",
            "## Risk",
            "- Low – documentation and metadata refresh only.",
            "",
            "## Commands",
            "- `python src/agent/main.py`",
        ]
    )
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def append_handoff(summary: Dict[str, Any], dry_run: bool) -> None:
    timestamp = summary["run_timestamp"]
    row = (
        f"\n| {timestamp} | agent | Auto-refresh analytics catalog | Maintain model/dataset freshness | "
        f"{'; '.join(summary.get('sources', [])) or 'internal catalog'} | python src/agent/main.py | "
        f"artifacts/agent/report.json | Review PR agent/update-{summary['run_date']} ({'dry-run' if dry_run else 'ready'}) |\n"
    )
    with HANDOFF.open("a", encoding="utf-8") as fh:
        fh.write(row)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--output", default="artifacts/agent/report.json")
    parser.add_argument("--pr-body", default="artifacts/agent/pr_body.md")
    args = parser.parse_args()

    run_ts = dt.datetime.now(dt.timezone.utc).replace(microsecond=0)
    run_date = run_ts.strftime("%Y%m%d")

    scanner = ModelScanner()
    catalog = scanner.scan()
    models = rank_candidates(catalog.get("models", []))
    datasets = catalog.get("datasets", [])
    issues = catalog.get("issues", [])
    tuning = catalog.get("tuning", [])
    metadata = catalog.get("metadata", {})

    doc_summary = update_docs(
        models=models,
        datasets=datasets,
        issues=issues,
        tuning=tuning,
        metadata=metadata,
        run_timestamp=run_ts,
        apply=not args.dry_run,
    )

    summary = {
        "run_timestamp": run_ts.isoformat().replace("+00:00", "Z"),
        "run_date": run_date,
        "models_count": len(models),
        "datasets_count": len(datasets),
        "placeholders_count": doc_summary.get("placeholders_count", 0),
        "issues_count": len(issues),
        "tuning_count": len(tuning),
        "sources": metadata.get("sources", []),
        "dry_run": args.dry_run,
    }

    write_json(Path(args.output), summary)
    write_pr_body(Path(args.pr_body), summary)
    append_handoff(summary, args.dry_run)


if __name__ == "__main__":
    main()
"""Package init."""
from __future__ import annotations

from typing import List


def _score(record: dict) -> float:
    latency = float(record.get("latency_fp16_ms") or record.get("latency_ms") or 1.0)
    accuracy = float(record.get("accuracy", 0.0))
    vram = float(record.get("vram_gb", 1.0))
    # Higher accuracy, lower latency/VRAM is better.
    return accuracy / (latency * (1.0 + vram * 0.1))


def rank_candidates(records: List[dict]) -> List[dict]:
    ranked = sorted(records, key=_score, reverse=True)
    for idx, rec in enumerate(ranked, start=1):
        rec["rank"] = idx
        rec["score"] = round(_score(rec), 6)
    return ranked
import numpy as np


def _dist(a: np.ndarray, b: np.ndarray) -> float:
    ax, ay = a
    bx, by = b
    return float(((ax - bx) ** 2 + (ay - by) ** 2) ** 0.5)


class PhoneUsage:
    """Heuristic detector for prolonged hand-to-ear proximity."""

    def __init__(
        self, hand_to_ear_px: float = 80.0, dwell_s: float = 2.0, fps: int = 15
    ) -> None:
        self.hand_to_ear_px = hand_to_ear_px
        self.dwell_frames = max(1, int(dwell_s * fps))
        self._counter = 0

    def score(self, keypoints: np.ndarray) -> tuple[float, bool]:
        idx = {"right_wrist": 10, "right_ear": 4, "left_wrist": 9, "left_ear": 3}
        try:
            right = _dist(keypoints[idx["right_wrist"]], keypoints[idx["right_ear"]])
            left = _dist(keypoints[idx["left_wrist"]], keypoints[idx["left_ear"]])
        except Exception:  # pragma: no cover - malformed keypoints
            return (0.0, False)
        near = (right < self.hand_to_ear_px) or (left < self.hand_to_ear_px)
        self._counter = self._counter + 1 if near else 0
        if self._counter >= self.dwell_frames:
            return (0.7, True)
        return (0.2 if near else 0.0, False)
import numpy as np


class GestureModel:
    """Predict simple hand gesture labels from pose keypoints."""

    LABELS = ("unknown", "halt", "proceed", "reverse", "caution")

    def __init__(self) -> None:
        pass

    @staticmethod
    def _y(kps: np.ndarray, idx: int) -> float:
        return float(kps[idx][1])

    def predict(self, kps: np.ndarray) -> tuple[str, float]:
        try:
            right_wrist = self._y(kps, 10)
            right_shoulder = self._y(kps, 6)
            left_wrist = self._y(kps, 9)
            left_shoulder = self._y(kps, 5)
        except Exception:  # pragma: no cover - malformed keypoints
            return ("unknown", 0.0)
        if right_wrist < right_shoulder - 10:
            return ("halt", 0.8)
        if left_wrist < left_shoulder - 10:
            return ("proceed", 0.7)
        return ("unknown", 0.1)
import numpy as np

try:  # pragma: no cover - optional dependency
    import onnxruntime as ort  # type: ignore[import-not-found]
except Exception:  # pragma: no cover - gracefully degrade when unavailable
    ort = None


class CollapseModel:
    """Score collapse severity using ONNX or heuristic fallback."""

    def __init__(self, onnx_path: str | None = None) -> None:
        self.session = None
        if onnx_path and ort is not None:
            providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
            try:
                self.session = ort.InferenceSession(onnx_path, providers=providers)
            except Exception:  # pragma: no cover - fallback to heuristic
                self.session = None
        self.min_drop = -1.8
        self.min_prone_px = 220

    def score(self, feature_window: list[dict[str, float]]) -> float:
        if not feature_window:
            return 0.0
        if self.session is not None:
            x = np.asarray(feature_window, dtype=np.float32)
            inp = self.session.get_inputs()[0].name
            out = self.session.get_outputs()[0].name
            return float(self.session.run([out], {inp: x})[0].squeeze())
        recent = feature_window[-3:]
        accel = np.mean([f.get("a_mag", 0.0) for f in recent])
        prone = np.mean([f.get("prone_height_px", self.min_prone_px) for f in recent])
        velocity = np.mean([f.get("v_mag", 0.0) for f in recent])
        score = 0.0
        if accel <= self.min_drop:
            score += 0.55
        if prone <= self.min_prone_px:
            score += 0.35
        if velocity < 3.0:
            score += 0.10
        return max(0.0, min(1.0, score))
"""Lightweight heuristics and wrappers for demo models."""
"""Tracking utilities (ByteTrack + manager helpers)."""

from __future__ import annotations

import os
from typing import Dict, List

from .bytetrack import ByteTrack, SimpleTracker, TrackState


class TrackerManager:
    """Manages per-camera trackers (ByteTrack by default)."""

    def __init__(
        self,
        algorithm: str = "bytetrack",
        high_thresh: float = 0.6,
        low_thresh: float = 0.1,
        match_iou: float = 0.3,
        max_age: int = 30,
        reid_engine=None,
    ) -> None:
        self.algorithm = algorithm.lower()
        self.trackers: Dict[str, object] = {}
        self.high_thresh = high_thresh
        self.low_thresh = low_thresh
        self.match_iou = match_iou
        self.max_age = max_age
        self.reid_engine = reid_engine
        self._cache: Dict[str, List[Dict[str, object]]] = {}

    def _build_tracker(self) -> object:
        if self.algorithm == "simple":
            return SimpleTracker(max_age=self.max_age, iou_thresh=self.match_iou)
        return ByteTrack(
            high_thresh=self.high_thresh,
            low_thresh=self.low_thresh,
            match_iou=self.match_iou,
            max_age=self.max_age,
            reid_engine=self.reid_engine,
        )

    def for_camera(self, camera_id: str) -> object:
        if camera_id not in self.trackers:
            self.trackers[camera_id] = self._build_tracker()
        return self.trackers[camera_id]

    def update(
        self, camera_id: str, detections: List[Dict[str, object]]
    ) -> List[Dict[str, object]]:
        tracker = self.for_camera(camera_id)
        updated = tracker.update(detections)
        self._cache[camera_id] = updated
        return updated

    def get_tracks(self, camera_id: str) -> List[Dict[str, object]]:
        tracker = self.trackers.get(camera_id)
        if tracker is None:
            return []
        return tracker.get_active_tracks()

    def latest_detections(self, camera_id: str) -> List[Dict[str, object]]:
        return self._cache.get(camera_id, [])


__all__ = ["ByteTrack", "SimpleTracker", "TrackerManager", "TrackState"]
"""
OSNet ReID embedding engine stub for ByteTrack integration.
"""

from typing import List
import numpy as np


class OSNetReID:
    def __init__(self, engine_path: str, threshold: float = 0.4):
        self.engine_path = engine_path
        self.threshold = threshold
        # TODO: Load TensorRT engine here

    def extract_embedding(self, image: np.ndarray) -> np.ndarray:
        # Deterministic, normalized embeddings for test
        if np.all(image == 0):
            emb = np.ones(256)
        elif np.all(image == 1):
            emb = np.full(256, 2.0)
        else:
            emb = np.full(256, 0.5)
        return emb / np.linalg.norm(emb)

    def match(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        # Cosine similarity
        return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))

    def is_match(self, emb1: np.ndarray, emb2: np.ndarray) -> bool:
        return self.match(emb1, emb2) > self.threshold
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple


def _iou(box_a: Sequence[float], box_b: Sequence[float]) -> float:
    ax1, ay1, ax2, ay2 = box_a
    bx1, by1, bx2, by2 = box_b
    inter_w = max(0.0, min(ax2, bx2) - max(ax1, bx1))
    inter_h = max(0.0, min(ay2, by2) - max(ay1, by1))
    inter = inter_w * inter_h
    if inter <= 0:
        return 0.0
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    return inter / (area_a + area_b - inter + 1e-6)


@dataclass
class TrackState:
    track_id: int
    bbox: List[float]
    score: float
    age: int = 0
    hits: int = 1
    active: bool = True


class ByteTrack:
    """Minimal ByteTrack-style tracker (greedy IoU matching with high/low thresholds)."""

    def __init__(
        self,
        high_thresh: float = 0.6,
        low_thresh: float = 0.1,
        match_iou: float = 0.3,
        max_age: int = 30,
        reid_engine=None,
    ) -> None:
        self.high_thresh = high_thresh
        self.low_thresh = low_thresh
        self.match_iou = match_iou
        self.max_age = max_age
        self._tracks: Dict[int, TrackState] = {}
        self._next_id = 1
        self.reid_engine = reid_engine

    def _spawn_track(self, det: Dict[str, object]) -> TrackState:
        tid = self._next_id
        self._next_id += 1
        track = TrackState(
            track_id=tid,
            bbox=list(det["bbox"]),
            score=float(det.get("confidence", 0.0)),
        )
        self._tracks[tid] = track
        det["track_id"] = tid
        det["first_seen"] = det.get("timestamp")
        return track

    def _match(
        self, tracks: Iterable[TrackState], detections: List[Dict[str, object]]
    ) -> None:
        unmatched_tracks: Dict[int, TrackState] = {t.track_id: t for t in tracks}
        for det in detections:
            best_score, best_id = 0.0, None
            for tid, state in list(unmatched_tracks.items()):
                iou = _iou(state.bbox, det["bbox"])
                score = iou
                # If ReID engine is available and embeddings present, use ReID for matching
                if (
                    self.reid_engine
                    and "embedding" in det
                    and "embedding" in state.__dict__
                ):
                    reid_score = self.reid_engine.match(
                        det["embedding"], state.__dict__["embedding"]
                    )
                    # Only match if similarity above threshold
                    if reid_score < self.reid_engine.threshold:
                        continue
                    score = max(score, reid_score)
                if score > best_score:
                    best_score = score
                    best_id = tid
            if best_score >= self.match_iou and best_id is not None:
                state = self._tracks[best_id]
                state.bbox = list(det["bbox"])
                state.score = float(det.get("confidence", state.score))
                state.age = 0
                state.hits += 1
                if "embedding" in det:
                    state.__dict__["embedding"] = det["embedding"]
                det["track_id"] = best_id
                unmatched_tracks.pop(best_id, None)
            else:
                det["track_id"] = None

    def update(self, detections: List[Dict[str, object]]) -> List[Dict[str, object]]:
        # Age tracks and drop stale ones
        for tid in list(self._tracks):
            state = self._tracks[tid]
            state.age += 1
            if state.age > self.max_age:
                del self._tracks[tid]

        high_conf = [
            det
            for det in detections
            if float(det.get("confidence", 0.0)) >= self.high_thresh
        ]
        low_conf = [
            det
            for det in detections
            if self.low_thresh <= float(det.get("confidence", 0.0)) < self.high_thresh
        ]

        self._match(self._tracks.values(), high_conf)

        # Spawn tracks for unmatched high-confidence detections
        for det in high_conf:
            if not det.get("track_id"):
                self._spawn_track(det)

        # Try to salvage low-confidence detections (keeps tracks alive)
        self._match(
            (
                state
                for state in self._tracks.values()
                if state.track_id not in [det.get("track_id") for det in high_conf]
            ),
            low_conf,
        )

        for det in low_conf:
            if not det.get("track_id"):
                continue
            state = self._tracks[det["track_id"]]
            state.bbox = list(det["bbox"])
            state.score = float(det.get("confidence", state.score))
            state.age = 0

        # Assign IDs back into detections if not already filled
        for det in detections:
            if det.get("track_id"):
                continue
            det["track_id"] = None
        return detections

    def get_active_tracks(self) -> List[Dict[str, object]]:
        return [
            {
                "track_id": state.track_id,
                "bbox": state.bbox,
                "confidence": state.score,
                "age": state.age,
                "hits": state.hits,
            }
            for state in self._tracks.values()
        ]


class SimpleTracker:
    """Kept for backwards compatibility / low-resource deployments."""

    def __init__(self, max_age: int = 30, iou_thresh: float = 0.3) -> None:
        self.max_age = max_age
        self.iou_thresh = iou_thresh
        self._tracks: Dict[int, TrackState] = {}
        self._next_id = 1

    def update(self, detections: List[Dict[str, object]]) -> List[Dict[str, object]]:
        for tid in list(self._tracks):
            self._tracks[tid].age += 1
            if self._tracks[tid].age > self.max_age:
                del self._tracks[tid]

        for det in detections:
            if det.get("track_id"):
                tid = int(det["track_id"])
                self._tracks[tid] = TrackState(
                    track_id=tid,
                    bbox=list(det["bbox"]),
                    score=float(det.get("confidence", 0.0)),
                )

        for det in detections:
            if det.get("track_id"):
                continue
            best_iou, best_id = 0.0, None
            for tid, state in self._tracks.items():
                iou = _iou(state.bbox, det["bbox"])
                if iou > best_iou:
                    best_iou = iou
                    best_id = tid
            if best_iou >= self.iou_thresh and best_id is not None:
                state = self._tracks[best_id]
                state.bbox = list(det["bbox"])
                state.score = float(det.get("confidence", state.score))
                state.age = 0
                det["track_id"] = best_id
            else:
                tid = self._next_id
                self._next_id += 1
                self._tracks[tid] = TrackState(
                    track_id=tid,
                    bbox=list(det["bbox"]),
                    score=float(det.get("confidence", 0.0)),
                )
                det["track_id"] = tid
        return detections

    def get_active_tracks(self) -> List[Dict[str, object]]:
        return [
            {
                "track_id": state.track_id,
                "bbox": state.bbox,
                "confidence": state.score,
                "age": state.age,
                "hits": state.hits,
            }
            for state in self._tracks.values()
        ]


__all__ = ["ByteTrack", "SimpleTracker", "TrackState"]
from __future__ import annotations

import json
import logging
import time
from pathlib import Path

from common.event_bus import Event, FileEventBus

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("tracker")


def stream_events(path: Path):
    offset = 0
    while True:
        if not path.exists():
            time.sleep(1)
            continue
        with path.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            for line in fh:
                offset = fh.tell()
                if not line.strip():
                    continue
                yield json.loads(line)
        time.sleep(0.2)


def main() -> None:
    source = Path("artifacts/detections/events.log")
    sink = FileEventBus(Path("artifacts/tracker"))
    for event in stream_events(source):
        e_type = event.get("type")
        payload = event.copy()
        payload.setdefault("track_id", payload.get("track_id", hash(str(payload)) % 1000))
        LOGGER.debug("Forwarding %s", e_type)
        sink.publish(Event(type=e_type, payload=payload))


if __name__ == "__main__":
    main()
from __future__ import annotations

import os
from pathlib import Path
from typing import Any

import yaml


def load_yaml(path: str | Path) -> Any:
    with Path(path).open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh)


def env_or_default(name: str, default: str | None = None) -> str:
    value = os.getenv(name)
    if value is None:
        if default is None:
            raise RuntimeError(f"Required environment variable {name} not set")
        return default
    return value


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p
"""Shared utilities for edge CCTV services."""
from __future__ import annotations

import json
import queue
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterator, Union

from .config import ensure_dir


@dataclass
class Event:
    type: str
    payload: Dict[str, Any]


class FileEventBus:
    """Minimal local event bus backed by newline-delimited JSON.

    Not for production but keeps services loosely coupled in demo profile.
    """

    def __init__(
        self,
        path: Union[Path, str],
        filename: str = "events.log",
        *,
        write_last: bool = True,
    ) -> None:
        directory = ensure_dir(Path(path))
        self.path = directory.joinpath(filename)
        self.last_event_path = directory.joinpath("last.json") if write_last else None
        self._lock = threading.Lock()
        self._queue: "queue.Queue[Event]" = queue.Queue()

    def publish(self, event: Event) -> None:
        self._queue.put(event)
        with self._lock:
            record = {"type": event.type, **event.payload}
            with self.path.open("a", encoding="utf-8") as fh:
                fh.write(json.dumps(record) + "\n")
            if self.last_event_path is not None:
                with self.last_event_path.open("w", encoding="utf-8") as fh:
                    json.dump(record, fh, indent=2)

    def consume(self) -> Iterator[Event]:
        while True:
            event = self._queue.get()
            yield event


__all__ = ["Event", "FileEventBus"]
"""Simplified ingest service for demo and testing."""
from __future__ import annotations

import json
import sys
from pathlib import Path

from src.common.config import ensure_dir


def main() -> None:
    base = ensure_dir("artifacts/ingest")
    events = base / "frames.log"
    if not events.exists():
        print("no events yet", file=sys.stderr)
        sys.exit(1)
    *_, last = events.read_text(encoding="utf-8").strip().splitlines()
    if not last:
        sys.exit(1)
    data = json.loads(last)
    if "frame_index" not in data:
        sys.exit(1)
    print("ok")


if __name__ == "__main__":
    main()
from __future__ import annotations

import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator

import cv2
import numpy as np

from src.common.config import ensure_dir


@dataclass
class Frame:
    index: int
    timestamp: float
    path: Path
    persist: bool = False


def synthetic_frames(output_dir: Path, fps: int = 25, persist: bool = False) -> Iterator[Frame]:
    """Generate alternating color frames for demo mode."""
    ensure_dir(output_dir)
    frame_idx = 0
    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]
    base = np.zeros((1080, 1920, 3), dtype=np.uint8)
    while True:
        frame = base.copy()
        color = colors[frame_idx % len(colors)]
        cv2.rectangle(frame, (200, 200), (1720, 880), color, thickness=-1)
        cv2.putText(
            frame,
            f"Frame {frame_idx}",
            (220, 300),
            cv2.FONT_HERSHEY_SIMPLEX,
            2.0,
            (255, 255, 255),
            4,
        )
        ts = time.time()
        path = output_dir / f"frame_{frame_idx:06d}.jpg"
        cv2.imwrite(str(path), frame)
        yield Frame(index=frame_idx, timestamp=ts, path=path, persist=persist)
        frame_idx += 1
        time.sleep(1.0 / fps)


def capture_rtsp(rtsp_url: str, output_dir: Path, fps_limit: int = 25, persist: bool = False) -> Iterator[Frame]:
    """Capture frames from RTSP using OpenCV."""
    ensure_dir(output_dir)
    cap = cv2.VideoCapture(rtsp_url)
    if not cap.isOpened():
        raise RuntimeError(f"Unable to open stream {rtsp_url}")
    frame_idx = 0
    last_ts = time.time()
    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(1)
            continue
        ts = time.time()
        if fps_limit:
            dt = ts - last_ts
            sleep_for = max(0, (1.0 / fps_limit) - dt)
            if sleep_for > 0:
                time.sleep(sleep_for)
        path = output_dir / f"frame_{frame_idx:06d}.jpg"
        cv2.imwrite(str(path), frame)
        yield Frame(index=frame_idx, timestamp=ts, path=path, persist=persist)
        frame_idx += 1
        last_ts = ts


def capture_webcam(device_index: int, output_dir: Path, fps_limit: int = 25, persist: bool = False) -> Iterator[Frame]:
    """Capture frames from a local webcam using OpenCV."""
    ensure_dir(output_dir)
    cap = cv2.VideoCapture(device_index)
    if not cap.isOpened():
        raise RuntimeError(f"Unable to open webcam index {device_index}")
    frame_idx = 0
    last_ts = time.time()
    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.5)
            continue
        ts = time.time()
        if fps_limit:
            dt = ts - last_ts
            sleep_for = max(0, (1.0 / fps_limit) - dt)
            if sleep_for > 0:
                time.sleep(sleep_for)
        path = output_dir / f"frame_{frame_idx:06d}.jpg"
        cv2.imwrite(str(path), frame)
        yield Frame(index=frame_idx, timestamp=ts, path=path, persist=persist)
        frame_idx += 1
        last_ts = ts
from __future__ import annotations

import argparse
import logging
import os
from pathlib import Path
from typing import Any, Dict

from src.common.config import load_yaml
from src.common.event_bus import Event, FileEventBus

from .pipeline import capture_rtsp, capture_webcam, synthetic_frames

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("ingest")


def normalize_source(source: str) -> str:
    if source.startswith("webcam://"):
        return source
    if source.startswith("webcam:"):
        return f"webcam://{source.split(':', 1)[1]}"
    return source


def resolve_camera(
    camera_config: Path | None,
    source: str | None,
    *,
    camera_id: str,
    camera_name: str | None,
    fps_limit: int | None,
) -> Dict[str, Any]:
    if source:
        return {
            "id": camera_id,
            "name": camera_name or camera_id,
            "rtsp_url": normalize_source(source),
            "fps_limit": fps_limit or 25,
        }
    if camera_config is None:
        raise RuntimeError("Provide --source or --camera-config.")
    config = load_yaml(camera_config) or {}
    cameras = config.get("cameras") or []
    if not cameras:
        raise RuntimeError(f"No cameras found in {camera_config}")
    camera = dict(cameras[0])
    camera.setdefault("id", camera_id)
    if camera_name:
        camera["name"] = camera_name
    if fps_limit is not None:
        camera["fps_limit"] = fps_limit
    return camera


def run(camera: Dict[str, Any], output_dir: Path, *, log_path: Path | None = None, persist_frames: bool = False) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    fps_limit = int(camera.get("fps_limit", 25))
    rtsp_url = camera.get("rtsp_url")
    bus_path = log_path or (output_dir / "frames.log")
    bus_path.parent.mkdir(parents=True, exist_ok=True)
    bus = FileEventBus(bus_path.parent, filename=bus_path.name)
    LOGGER.info("Starting ingest stream", extra={"camera": camera.get("id")})
    if not rtsp_url or rtsp_url.startswith("demo://"):
        generator = synthetic_frames(output_dir, fps=fps_limit, persist=persist_frames)
    elif rtsp_url.startswith("webcam://"):
        try:
            index = int(rtsp_url.split("://", 1)[1])
        except ValueError as exc:  # pragma: no cover - invalid string
            raise RuntimeError(f"Invalid webcam URL: {rtsp_url}") from exc
        generator = capture_webcam(index, output_dir, fps_limit, persist=persist_frames)
    else:
        generator = capture_rtsp(rtsp_url, output_dir, fps_limit, persist=persist_frames)
    for frame in generator:
        bus.publish(
            Event(
                type="frame",
                payload={
                    "camera_id": camera.get("id", "CAM01"),
                    "frame_index": frame.index,
                    "timestamp": frame.timestamp,
                    "path": str(frame.path),
                    "persist": frame.persist,
                },
            )
        )
        LOGGER.debug("Captured frame %s", frame.index)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--camera-config",
        default=os.getenv("CAMERA_CONFIG", "configs/cameras.yaml"),
        help="Path to cameras YAML (defaults to $CAMERA_CONFIG or configs/cameras.yaml when --source not provided).",
    )
    parser.add_argument(
        "--source",
        help="Override source URI (webcam:0, demo://synthetic, rtsp://...).",
    )
    parser.add_argument(
        "--camera-id",
        default=os.getenv("CAMERA_ID", "CAM01"),
        help="Camera ID used when emitting frames (default: %(default)s).",
    )
    parser.add_argument(
        "--camera-name",
        help="Optional camera display name override.",
    )
    parser.add_argument(
        "--fps",
        type=int,
        help="FPS cap override applied to the selected source.",
    )
    parser.add_argument(
        "--output",
        dest="output",
        default=os.getenv("INGEST_OUTPUT", "artifacts/ingest"),
        help="Directory for captured frames.",
    )
    parser.add_argument(
        "--out-dir",
        dest="output",
        help="Alias for --output.",
    )
    parser.add_argument(
        "--log-path",
        default=os.getenv("INGEST_FRAMES_LOG"),
        help="Optional path for the frames log (defaults to <output>/frames.log).",
    )
    parser.add_argument(
        "--save-frames",
        action="store_true",
        help="Persist captured frames on disk (off by default; files are deleted after consumption).",
    )
    args = parser.parse_args()

    camera_cfg = Path(args.camera_config).resolve() if args.camera_config and not args.source else None
    camera = resolve_camera(
        camera_cfg,
        args.source,
        camera_id=args.camera_id,
        camera_name=args.camera_name,
        fps_limit=args.fps,
    )
    log_path = Path(args.log_path).resolve() if args.log_path else None
    run(
        camera,
        Path(args.output).resolve(),
        log_path=log_path,
        persist_frames=bool(args.save_frames),
    )


if __name__ == "__main__":
    main()
"""Rule evaluation engine for CCTV analytics."""
from __future__ import annotations

import math
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional

import yaml

from common.event_bus import Event
from aixavier.core.usecases import UseCaseRegistry, UseCaseDefinition


@dataclass
class RuleConfig:
    type: str
    params: Dict[str, float | int | str | List[str] | Dict[str, str]] = field(default_factory=dict)


@dataclass
class UseCase:
    metadata: Dict[str, str]
    rules: List[RuleConfig]
    definition: Optional[UseCaseDefinition] = None


class RuleEngine:
    """Evaluates configured rules on detection events."""

    def __init__(self, configs_dir: Path, manifest_path: Optional[Path] = None):
        self.configs_dir = Path(configs_dir)
        manifest = manifest_path or Path("assets/usecases/catalog.yaml")
        self.registry = UseCaseRegistry(manifest_path=manifest, configs_dir=self.configs_dir)
        self.usecases = self._load_usecases(self.configs_dir)
        self.state: Dict[str, Dict[str, float]] = {}

    def _load_usecases(self, path: Path) -> Dict[str, UseCase]:
        raw_configs: Dict[str, Dict[str, object]] = {}
        for file in path.glob("*.yaml"):
            data = yaml.safe_load(file.read_text(encoding="utf-8")) or {}
            metadata = data.get("metadata", {})
            slug = metadata.get("id", file.stem)
            rules = [RuleConfig(type=rule.pop("type"), params=rule) for rule in data.get("rules", [])]
            raw_configs[slug] = {"metadata": metadata, "rules": rules}

        mapping: Dict[str, UseCase] = {}
        for definition in self.registry.definitions:
            payload = raw_configs.pop(definition.slug, {"metadata": {}, "rules": []})
            metadata = self.registry.enrich_metadata(definition.slug, payload.get("metadata", {}))
            rules = payload.get("rules", [])
            mapping[definition.slug] = UseCase(metadata=metadata, rules=rules, definition=definition)

        for slug, payload in raw_configs.items():
            metadata = self.registry.enrich_metadata(slug, payload.get("metadata", {}))
            mapping[slug] = UseCase(metadata=metadata, rules=payload.get("rules", []), definition=self.registry.get(slug))

        return mapping

    def summary(self) -> Dict[str, Dict[str, object]]:
        report: Dict[str, Dict[str, object]] = {}
        for slug, usecase in self.usecases.items():
            maturity = usecase.metadata.get(
                "maturity",
                usecase.definition.maturity if usecase.definition else "planned",
            )
            report[slug] = {
                "rules": len(usecase.rules),
                "maturity": maturity,
            }
        return report

    def evaluate(self, event: Dict[str, any]) -> List[Event]:
        """Evaluate event against all rules and return triggered events."""
        triggered: List[Event] = []
        for usecase in self.usecases.values():
            for rule in usecase.rules:
                handler = getattr(self, f"_handle_{rule.type}", None)
                if handler is None:
                    continue
                result = handler(event, rule.params, usecase)
                if result:
                    triggered.append(result)
        return triggered

    # --- Rule handlers ---
    def _handle_line_crossing(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "track":
            return None
        if event.get("class") not in params.get("classes", []):
            return None
        if event.get("line_id") != params.get("line_id"):
            return None
        return Event(
            type=usecase.metadata["id"],
            payload={
                "confidence": event.get("confidence", 0.0),
                "track_id": event.get("track_id"),
                "camera_id": event.get("camera_id"),
                "attributes": {"direction": event.get("direction")},
            },
        )

    def _handle_static_object_dwell(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "object" or event.get("class") not in params.get("classes", []):
            return None
        key = f"{event.get('camera_id')}::{event.get('track_id')}"
        event_ts = float(event.get("timestamp", time.time()))
        wall_clock = time.time()
        dwell_seconds = float(params.get("dwell_seconds", 30))
        dwell_state = self.state.setdefault("dwell", {})

        if "first_seen" in event:
            dwell_state[key] = float(event["first_seen"])
        elif key not in dwell_state:
            dwell_state[key] = min(event_ts, wall_clock)
        else:
            dwell_state[key] = min(float(dwell_state[key]), event_ts)

        first_seen = float(dwell_state[key])
        elapsed = max(event_ts, wall_clock) - first_seen
        if elapsed >= dwell_seconds:
            return Event(
                type=usecase.metadata["id"],
                payload={
                    "camera_id": event.get("camera_id"),
                    "track_id": event.get("track_id"),
                    "dwell_seconds": elapsed,
                    "confidence": event.get("confidence", 0.0),
                },
            )
        return None

    def _handle_action_score(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "action":
            return None
        label = params.get("label")
        threshold = float(params.get("threshold", 0.5))
        score = event.get("scores", {}).get(label, 0.0)
        if score >= threshold:
            return Event(
                type=usecase.metadata["id"],
                payload={
                    "camera_id": event.get("camera_id"),
                    "track_id": event.get("track_id"),
                    "action": label,
                    "score": score,
                },
            )
        return None

    def _handle_pose_velocity_drop(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "pose":
            return None
        velocity = event.get("velocity_drop", 0.0)
        threshold = float(params.get("min_drop", 1.5))
        if velocity >= threshold:
            return Event(
                type=usecase.metadata["id"],
                payload={
                    "camera_id": event.get("camera_id"),
                    "track_id": event.get("track_id"),
                    "velocity_drop": velocity,
                },
            )
        return None

    def _handle_prone_dwell(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "pose":
            return None
        height = event.get("height_px", math.inf)
        max_height = params.get("max_height_px", 200)
        dwell = event.get("dwell_seconds", 0)
        if height <= max_height and dwell >= params.get("min_duration_seconds", 4):
            return Event(
                type=usecase.metadata["id"],
                payload={
                    "camera_id": event.get("camera_id"),
                    "track_id": event.get("track_id"),
                    "dwell_seconds": dwell,
                },
            )
        return None

    def _handle_frs_match(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "frs":
            return None
        score = event.get("score", 0.0)
        threshold = float(params.get("threshold", 0.47))
        if score >= threshold:
            return Event(
                type=usecase.metadata["id"],
                payload={
                    "camera_id": event.get("camera_id"),
                    "identity": event.get("identity"),
                    "score": score,
                },
            )
        return None

    def _handle_blur_detect(self, event: Dict[str, any], params: Dict[str, any], usecase: UseCase) -> Optional[Event]:
        if event.get("type") != "tamper":
            return None
        variance = event.get("variance", 0.0)
        threshold = float(params.get("variance_threshold", 100))
        if variance <= threshold:
            return Event(
                type=usecase.metadata["id"],
                payload={"variance": variance, "camera_id": event.get("camera_id")},
            )
        return None


__all__ = ["RuleEngine"]
from __future__ import annotations

import json
import logging
import time
from pathlib import Path

from common.config import env_or_default
from common.event_bus import Event, FileEventBus

from .engine import RuleEngine

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("rules")


def stream_events(source: Path):
    offset = 0
    while True:
        if not source.exists():
            time.sleep(1)
            continue
        with source.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            for line in fh:
                offset = fh.tell()
                if not line.strip():
                    continue
                yield json.loads(line)
        time.sleep(0.2)


def main() -> None:
    configs_dir = Path(env_or_default("RULES_DIR", "configs/usecases"))
    profile_config = Path(env_or_default("PROFILE_CONFIG", "configs/profile_demo.yaml"))
    profile = profile_config.read_text(encoding="utf-8")
    LOGGER.info("Loading rule engine with profile %s", profile_config.name)
    engine = RuleEngine(configs_dir)
    source_log = Path("artifacts/tracker/events.log")
    sink = FileEventBus(Path("artifacts/events"))
    for event in stream_events(source_log):
        for triggered in engine.evaluate(event):
            LOGGER.info("Triggered %s", triggered.type)
            sink.publish(Event(type=triggered.type, payload=triggered.payload))


if __name__ == "__main__":
    main()
"""Package init."""
"""
Circular video buffer stub for event-driven recording.
"""

from collections import deque
from typing import Any, Deque, Optional


class CircularBuffer:
    def __init__(self, max_frames: int = 300):
        self.max_frames = max_frames
        self.buffer: Deque[Any] = deque(maxlen=max_frames)

    def add_frame(self, frame: Any) -> None:
        self.buffer.append(frame)

    def get_clip(self, pre: int = 30, post: int = 60) -> list[Any]:
        # Return last `pre` frames and next `post` frames (stub: just returns all)
        return list(self.buffer)

    def clear(self) -> None:
        self.buffer.clear()

    def __len__(self) -> int:
        return len(self.buffer)
from __future__ import annotations

import json
import logging
import time
from pathlib import Path

from common.config import ensure_dir

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("recorder")


def stream_events(path: Path):
    offset = 0
    while True:
        if not path.exists():
            time.sleep(1)
            continue
        with path.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            for line in fh:
                offset = fh.tell()
                if not line.strip():
                    continue
                yield json.loads(line)
        time.sleep(0.5)


def main() -> None:
    source = Path("artifacts/events/events.log")
    recordings_dir = ensure_dir("artifacts/recordings")
    for event in stream_events(source):
        clip_path = recordings_dir / f"{event.get('type')}_{int(time.time())}.json"
        clip_path.write_text(json.dumps(event, indent=2), encoding="utf-8")
        LOGGER.info("Recorded artifact %s", clip_path)


if __name__ == "__main__":
    main()
"""Package init."""
from __future__ import annotations

import json
import os
import time
from pathlib import Path
from typing import Dict

from prometheus_client import Counter, Gauge, start_http_server

INGEST_FPS = Gauge("ingest_fps", "Ingest frames per second", ["camera"])
DETECT_LATENCY = Gauge("detect_latency_ms", "Detector latency by use case", ["usecase"])
DETECT_LATENCY_BY_MODEL = Gauge(
    "detector_latency_model_ms",
    "Detector latency averaged per model",
    ["model"],
)
EVENT_COUNT = Counter("event_count", "Event count", ["type"])
POSE_EVENT_COUNT = Counter("pose_event_total", "Pose events published", ["type"])
POSE_EVENT_RATE = Gauge("pose_events_per_second", "Pose event rate (per polling window)", ["camera"])
FRAME_QUEUE_DEPTH = Gauge("frame_queue_depth", "Approx backlog between ingest and detector")
GPU_UTIL = Gauge("gpu_util", "Simulated GPU utilization")
MEM_USED = Gauge("mem_used_bytes", "Simulated memory usage")
DROP_FRAMES = Counter("drop_frames_total", "Dropped frames total")


def count_events(path: Path) -> Dict[str, int]:
    pose_counts: Dict[str, int] = {}
    if not path.exists():
        return pose_counts
    lines = path.read_text(encoding="utf-8").splitlines()
    for line in lines[-50:]:
        event = json.loads(line)
        event_type = event.get("type", "unknown")
        EVENT_COUNT.labels(type=event_type).inc()
        if event_type in {"pose", "pose_velocity"}:
            POSE_EVENT_COUNT.labels(type=event_type).inc()
            camera = event.get("camera_id", "unknown")
            pose_counts[camera] = pose_counts.get(camera, 0) + 1
        latency = float(event.get("latency_ms", 0.0))
        detector = event.get("detector", event_type)
        if latency:
            DETECT_LATENCY.labels(usecase=event_type).set(latency)
            DETECT_LATENCY_BY_MODEL.labels(model=detector).set(latency)
    return pose_counts


def read_latest_frame_index(path: Path, key: str) -> int:
    if not path.exists():
        return 0
    try:
        lines = path.read_text(encoding="utf-8").splitlines()
    except OSError:
        return 0
    for line in reversed(lines):
        if not line.strip():
            continue
        try:
            payload = json.loads(line)
        except json.JSONDecodeError:
            continue
        value = payload.get(key)
        if isinstance(value, int):
            return value
    return 0


def main() -> None:
    port = int(os.getenv("PROMETHEUS_SCRAPE_PORT", "9100"))
    start_http_server(port)
    while True:
        INGEST_FPS.labels(camera="CAM01").set(25)
        DETECT_LATENCY.labels(usecase="trespassing_on_track").set(8.0)
        GPU_UTIL.set(0.42)
        MEM_USED.set(2_147_483_648)
        pose_counts = count_events(Path("artifacts/events/events.log"))
        for camera, count in pose_counts.items():
            POSE_EVENT_RATE.labels(camera=camera).set(count / 5.0)
        frames_idx = read_latest_frame_index(Path("artifacts/ingest/frames.log"), "frame_index")
        processed_idx = read_latest_frame_index(Path("artifacts/detections/events.log"), "frame_index")
        FRAME_QUEUE_DEPTH.set(max(0, frames_idx - processed_idx))
        time.sleep(5)


if __name__ == "__main__":
    main()
from __future__ import annotations

from typing import Iterable, List, Sequence


def _bbox_iou(a: Sequence[float], b: Sequence[float]) -> float:
    ax1, ay1, ax2, ay2 = a
    bx1, by1, bx2, by2 = b
    inter_w = max(0.0, min(ax2, bx2) - max(ax1, bx1))
    inter_h = max(0.0, min(ay2, by2) - max(ay1, by1))
    inter = inter_w * inter_h
    if inter <= 0:
        return 0.0
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    return inter / (area_a + area_b - inter + 1e-6)


def associate_pose_tracks(
    persons: Iterable[dict],
    poses: List[dict],
    min_iou: float = 0.1,
) -> List[dict]:
    person_list = list(persons)
    if not person_list:
        return poses
    for pose in poses:
        best_iou, best_person = 0.0, None
        for person in person_list:
            bbox = person.get("bbox")
            if not bbox or not pose.get("bbox"):
                continue
            score = _bbox_iou(bbox, pose["bbox"])
            if score > best_iou:
                best_iou = score
                best_person = person
        if best_person and best_iou >= min_iou:
            pose["track_id"] = best_person.get("track_id")
            if best_person.get("first_seen") is not None:
                pose["first_seen"] = best_person["first_seen"]
    return poses


__all__ = ["associate_pose_tracks"]
"""
Rendering helpers for pose detections (skeleton, bbox, HUD overlays).
"""

from __future__ import annotations

from typing import Iterable, List, Sequence, Tuple

import cv2
import numpy as np

# Edges for COCO-17 style skeletons.
COCO_EDGES: List[Tuple[int, int]] = [
    (5, 7),
    (7, 9),
    (6, 8),
    (8, 10),
    (5, 6),
    (5, 11),
    (6, 12),
    (11, 12),
    (11, 13),
    (13, 15),
    (12, 14),
    (14, 16),
    (0, 5),
    (0, 6),
]


def draw_pose(
    image: np.ndarray,
    keypoints: np.ndarray,
    bbox: Sequence[float] | None = None,
    threshold: float = 0.25,
) -> np.ndarray:
    """Draws joints, skeleton edges, and (optionally) bounding box."""
    overlay = image
    h, w = overlay.shape[:2]
    kpts = np.asarray(keypoints, dtype=np.float32)
    if kpts.ndim != 2 or kpts.shape[1] < 3:
        return overlay
    for x, y, conf in kpts:
        if conf < threshold:
            continue
        if 0 <= x < w and 0 <= y < h:
            cv2.circle(overlay, (int(x), int(y)), 3, (0, 255, 0), -1, lineType=cv2.LINE_AA)
    for u, v in COCO_EDGES:
        if u >= len(kpts) or v >= len(kpts):
            continue
        x1, y1, c1 = kpts[u]
        x2, y2, c2 = kpts[v]
        if c1 < threshold or c2 < threshold:
            continue
        if not (0 <= x1 < w and 0 <= y1 < h and 0 <= x2 < w and 0 <= y2 < h):
            continue
        cv2.line(
            overlay,
            (int(x1), int(y1)),
            (int(x2), int(y2)),
            (0, 200, 255),
            2,
            lineType=cv2.LINE_AA,
        )
    if bbox is not None and len(bbox) == 4:
        x1, y1, x2, y2 = map(int, bbox)
        cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 100, 0), 2, lineType=cv2.LINE_AA)
    return overlay


def draw_hud(image: np.ndarray, lines: Iterable[str], origin: Tuple[int, int] = (10, 20)) -> np.ndarray:
    """Writes multi-line heads-up display text."""
    x, y = origin
    for line in lines:
        cv2.putText(
            image,
            line,
            (x, y),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (255, 255, 255),
            2,
            lineType=cv2.LINE_AA,
        )
        y += 22
    return image


def draw_track_label(
    image: np.ndarray,
    bbox: Sequence[float] | None,
    label: str,
    color: Tuple[int, int, int] = (0, 255, 255),
) -> np.ndarray:
    if not label:
        return image
    h, w = image.shape[:2]
    if bbox and len(bbox) == 4:
        x = int(max(0, min(w - 1, bbox[0])))
        y = int(max(0, min(h - 1, bbox[1] - 8)))
    else:
        x, y = 10, 10
    cv2.putText(
        image,
        label,
        (x, y),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.6,
        color,
        2,
        lineType=cv2.LINE_AA,
    )
    return image


__all__ = ["draw_pose", "draw_hud", "draw_track_label"]
from __future__ import annotations

import os
from typing import List, Sequence

if os.environ.get("AIXAVIER_DISABLE_NUMPY") == "1":  # test fallback path
    np = None
else:  # pragma: no cover - actual runtime path
    try:
        import numpy as np
    except Exception:  # pragma: no cover - runtime without numpy
        np = None


class SimpleArray:
    def __init__(self, data: Sequence[Sequence[float]]) -> None:
        self.data = data
        self.shape = self._compute_shape(data)
        self.ndim = len(self.shape)

    @staticmethod
    def _compute_shape(data: Sequence) -> tuple:
        shape = []
        ref = data
        while isinstance(ref, (list, tuple)):
            shape.append(len(ref))
            if len(ref) == 0:
                break
            ref = ref[0]
        return tuple(shape)

    def __getitem__(self, item):  # pragma: no cover - debugging helper
        result = self.data[item]
        return SimpleArray(result) if isinstance(result, (list, tuple)) else result


def _transpose_021(
    data: Sequence[Sequence[Sequence[float]]],
) -> List[List[List[float]]]:
    dim0 = len(data)
    dim1 = len(data[0]) if dim0 else 0
    dim2 = len(data[0][0]) if dim1 else 0
    result: List[List[List[float]]] = []
    for i in range(dim0):
        anchors: List[List[float]] = []
        for k in range(dim2):
            row = [data[i][j][k] for j in range(dim1)]
            anchors.append(row)
        result.append(anchors)
    return result


def xywh_to_xyxy(box: "np.ndarray") -> "np.ndarray":  # type: ignore[valid-type]
    if np is None:
        raise RuntimeError("numpy is required for detector execution")
    x, y, w, h = box.T
    return np.stack((x - w / 2, y - h / 2, x + w / 2, y + h / 2), axis=1)


def nms(boxes: "np.ndarray", scores: "np.ndarray", iou_thres: float) -> List[int]:  # type: ignore[valid-type]
    if np is None:
        raise RuntimeError("numpy is required for detector execution")
    if boxes.size == 0:
        return []
    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]
    keep: List[int] = []
    while order.size > 0:
        i = order[0]
        keep.append(int(i))
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        inds = np.where(ovr <= iou_thres)[0]
        order = order[inds + 1]
    return keep


def normalise_object_output(out):
    if np is not None:
        arr = np.asarray(out)
        if arr.ndim == 3:
            if arr.shape[1] < arr.shape[2]:
                arr = np.transpose(arr, (0, 2, 1))
            arr = arr[0]
        elif arr.ndim != 2:
            raise RuntimeError(f"Unexpected detector output shape: {arr.shape}")
        if arr.shape[1] < 6:
            raise RuntimeError(
                f"Detector output has insufficient features: {arr.shape}"
            )
        return arr

    data = out
    if isinstance(data, SimpleArray):
        arr = data
    else:
        arr = SimpleArray(data)
    if arr.ndim == 3:
        nested = arr.data
        if arr.shape[1] < arr.shape[2]:
            nested = _transpose_021(nested)
        nested = nested[0]
        arr = SimpleArray(nested)
    elif arr.ndim != 2:
        raise RuntimeError(f"Unexpected detector output structure: {arr.shape}")
    if arr.shape[1] < 6:
        raise RuntimeError(f"Detector output has insufficient features: {arr.shape}")
    return arr


def normalise_pose_output(out):
    if np is not None:
        arr = np.asarray(out)
        if arr.ndim == 3:
            if arr.shape[1] < arr.shape[2]:
                arr = np.transpose(arr, (0, 2, 1))
            arr = arr[0]
        elif arr.ndim != 2:
            raise RuntimeError(f"Unexpected pose output shape: {arr.shape}")
        if arr.shape[1] < 6:
            raise RuntimeError(f"Pose output has insufficient features: {arr.shape}")
        return arr

    data = out
    if isinstance(data, SimpleArray):
        arr = data
    else:
        arr = SimpleArray(data)
    if arr.ndim == 3:
        nested = arr.data
        if arr.shape[1] < arr.shape[2]:
            nested = _transpose_021(nested)
        nested = nested[0]
        arr = SimpleArray(nested)
    elif arr.ndim != 2:
        raise RuntimeError(f"Unexpected pose output structure: {arr.shape}")
    if arr.shape[1] < 6:
        raise RuntimeError(f"Pose output has insufficient features: {arr.shape}")
    return arr


__all__ = [
    "xywh_to_xyxy",
    "nms",
    "normalise_object_output",
    "normalise_pose_output",
]


def _ema(previous, current, alpha: float = 0.4):
    if previous is None:
        return current
    return alpha * current + (1 - alpha) * previous


class KeypointSmoother:
    """Exponential moving average smoother for pose keypoints."""

    def __init__(self, alpha: float = 0.4) -> None:
        self.alpha = alpha
        self._previous = None

    def __call__(self, keypoints):
        self._previous = _ema(self._previous, keypoints, self.alpha)
        return self._previous
"""Package init."""
from __future__ import annotations

import logging
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import cv2
import numpy as np

from .postprocess import (
    normalise_object_output,
    normalise_pose_output,
    nms,
    xywh_to_xyxy,
)

LOGGER = logging.getLogger(__name__)

_ORT = None  # lazy-loaded onnxruntime module


def _sync_session_dims(session: Any, input_w: int, input_h: int) -> Tuple[int, int]:
    """Derive spatial dimensions from ONNX model if available."""
    if session is None:
        return input_w, input_h
    try:
        shape = session.get_inputs()[0].shape
    except (IndexError, AttributeError):
        return input_w, input_h
    if not shape or len(shape) < 4:
        return input_w, input_h
    _, _, h_dim, w_dim = shape[:4]

    def _coerce(value: Any, fallback: int) -> int:
        if isinstance(value, (int, float)) and value > 0:
            return int(value)
        return fallback

    inferred_h = _coerce(h_dim, input_h)
    inferred_w = _coerce(w_dim, input_w)
    return inferred_w, inferred_h


def letterbox(
    image: np.ndarray, new_shape: Tuple[int, int]
) -> Tuple[np.ndarray, float, Tuple[int, int]]:
    """Resize image with unchanged aspect ratio using padding (YOLO-style)."""
    height, width = image.shape[:2]
    new_w, new_h = new_shape
    scale = min(new_w / width, new_h / height)
    resized = cv2.resize(
        image, (int(width * scale), int(height * scale)), interpolation=cv2.INTER_LINEAR
    )
    top = int((new_h - resized.shape[0]) / 2)
    left = int((new_w - resized.shape[1]) / 2)
    canvas = np.full((new_h, new_w, 3), 114, dtype=np.uint8)
    canvas[top : top + resized.shape[0], left : left + resized.shape[1]] = resized
    return canvas, scale, (left, top)


def load_onnx_session(model_path: Path) -> Optional[Any]:
    global _ORT
    if _ORT is None:
        try:  # pragma: no cover - optional dependency
            import onnxruntime as ort_mod
        except Exception:
            ort_mod = None
        _ORT = ort_mod

    if _ORT is None:
        LOGGER.warning(
            "onnxruntime not available; falling back to heuristic detections."
        )
        return None
    if not model_path.exists():
        LOGGER.warning(
            "ONNX model %s not found; falling back to heuristic detections.", model_path
        )
        return None
    providers = (
        ["CUDAExecutionProvider", "CPUExecutionProvider"]
        if "CUDAExecutionProvider" in _ORT.get_available_providers()
        else ["CPUExecutionProvider"]
    )
    try:
        return _ORT.InferenceSession(str(model_path), providers=providers)
    except Exception as exc:  # pragma: no cover - runtime specific
        LOGGER.error(
            "Failed to load ONNX model %s (%s); falling back to heuristic detections.",
            model_path,
            exc,
        )
        return None


@dataclass
class Detection:
    bbox: Tuple[float, float, float, float]
    score: float
    class_id: int
    class_name: str


@dataclass
class PoseDetection:
    bbox: Tuple[float, float, float, float]
    score: float
    keypoints: List[Tuple[float, float, float]]  # (x, y, confidence)


class BaseDetector:
    event_type: str = "object"

    def __init__(self, config: Dict[str, Any]) -> None:
        self.config = config

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        raise NotImplementedError


class ObjectDetector(BaseDetector):
    event_type = "object"

    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.input_w = int(config.get("input", {}).get("width", 640))
        self.input_h = int(config.get("input", {}).get("height", 640))
        self.conf_thres = float(config.get("confidence_threshold", 0.25))
        self.iou_thres = float(config.get("nms_iou_threshold", 0.5))
        self.max_det = int(config.get("max_detections", 300))
        self.classes = config.get("classes") or []
        onnx_path = config.get("onnx_path")
        self.session = load_onnx_session(Path(onnx_path)) if onnx_path else None
        if self.session:
            self.input_name = self.session.get_inputs()[0].name
        else:
            self.input_name = "images"
        self.input_w, self.input_h = _sync_session_dims(
            self.session, self.input_w, self.input_h
        )
        self.frame_shape: Optional[Tuple[int, int]] = None
        LOGGER.info("Object detector ready; ONNX=%s", bool(self.session))

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        self.frame_shape = image.shape[:2]
        if self.session:
            return self._detect_onnx(image)
        return self._detect_heuristic(image)

    def _detect_onnx(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        img, scale, pad = letterbox(image, (self.input_w, self.input_h))
        blob = img.transpose(2, 0, 1).astype(np.float32) / 255.0
        blob = np.expand_dims(blob, 0)
        outputs = self.session.run(None, {self.input_name: blob})[0]
        outputs = normalise_object_output(outputs)
        boxes = outputs[:, :4]
        scores = outputs[:, 4:]
        conf = np.max(scores, axis=1)
        class_ids = np.argmax(scores, axis=1)
        mask = conf >= self.conf_thres
        boxes, conf, class_ids = boxes[mask], conf[mask], class_ids[mask]
        if boxes.size == 0:
            return []
        boxes = xywh_to_xyxy(boxes)
        # Undo letterbox
        boxes -= np.array([pad[0], pad[1], pad[0], pad[1]])
        boxes /= scale
        keep = nms(boxes, conf, self.iou_thres)
        if self.max_det and len(keep) > self.max_det:
            ranked = sorted(keep, key=lambda idx: conf[idx], reverse=True)[
                : self.max_det
            ]
            keep = ranked
        detections: List[Dict[str, Any]] = []
        for idx in keep:
            x1, y1, x2, y2 = boxes[idx]
            class_id = int(class_ids[idx])
            class_name = (
                self.classes[class_id]
                if class_id < len(self.classes)
                else str(class_id)
            )
            detections.append(
                {
                    "bbox": [float(x1), float(y1), float(x2), float(y2)],
                    "confidence": float(conf[idx]),
                    "class_id": class_id,
                    "class": class_name,
                }
            )
        return detections

    def _detect_heuristic(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        h, w = image.shape[:2]
        size = min(h, w) * 0.4
        x1 = (w - size) / 2
        y1 = (h - size) / 2
        x2 = x1 + size
        y2 = y1 + size
        class_name = self.classes[0] if self.classes else "object"
        LOGGER.debug("Using heuristic bounding box for %s", class_name)
        return [
            {
                "bbox": [float(x1), float(y1), float(x2), float(y2)],
                "confidence": 0.5,
                "class_id": 0,
                "class": class_name,
            }
        ]


class PoseDetector(BaseDetector):
    event_type = "pose"

    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.input_w = int(config.get("input", {}).get("width", 640))
        self.input_h = int(config.get("input", {}).get("height", 640))
        self.conf_thres = float(config.get("confidence_threshold", 0.25))
        self.iou_thres = float(config.get("nms_iou_threshold", 0.6))
        self.max_det = int(config.get("max_detections", 200))
        onnx_path = config.get("onnx_path")
        self.session = load_onnx_session(Path(onnx_path)) if onnx_path else None
        if self.session:
            self.input_name = self.session.get_inputs()[0].name
        else:
            self.input_name = "images"
        self.input_w, self.input_h = _sync_session_dims(
            self.session, self.input_w, self.input_h
        )
        person_cfg = config.get("person_detector") or {}
        if person_cfg:
            self.person_detector = ObjectDetector(person_cfg)
            class_filter = (
                person_cfg.get("classes_filter")
                or person_cfg.get("classes")
                or ["person"]
            )
            self.person_class_filter = {str(label).lower() for label in class_filter}
            self.person_min_conf = float(person_cfg.get("confidence_threshold", 0.4))
            self.person_padding_ratio = float(
                person_cfg.get("crop_padding_ratio", 0.15)
            )
        else:
            self.person_detector = None
            self.person_class_filter = set()
            self.person_min_conf = 0.0
            self.person_padding_ratio = 0.0
        LOGGER.info("Pose detector ready; ONNX=%s", bool(self.session))

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        if not self.session:
            return self._detect_heuristic(image)
        if self.person_detector is not None:
            person_boxes = self._person_boxes(image)
            if person_boxes:
                detections: List[Dict[str, Any]] = []
                for bbox in person_boxes:
                    detections.extend(self._detect_from_person(image, bbox))
                if detections:
                    return detections
        return self._detect_full_frame(image)

    def _person_boxes(self, image: np.ndarray) -> List[List[float]]:
        boxes: List[List[float]] = []
        for det in self.person_detector.detect(image):
            class_name = str(det.get("class", "")).lower()
            if class_name not in self.person_class_filter:
                continue
            if float(det.get("confidence", 0.0)) < self.person_min_conf:
                continue
            bbox = det.get("bbox")
            if not bbox or len(bbox) != 4:
                continue
            boxes.append([float(x) for x in bbox])
        return boxes

    def _decode_simcc(
        self, simcc_x: Any, simcc_y: Any
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        if np is None:
            raise RuntimeError("numpy is required for pose detection")
        simcc_x = np.asarray(simcc_x)
        simcc_y = np.asarray(simcc_y)

        def _prepare(arr: np.ndarray) -> np.ndarray:
            if arr.ndim == 2:
                arr = arr[None, ...]
            if arr.ndim != 3:
                raise RuntimeError(f"Unexpected SimCC tensor shape: {arr.shape}")
            return arr

        simcc_x = _prepare(simcc_x)
        simcc_y = _prepare(simcc_y)
        if simcc_x.shape[:2] != simcc_y.shape[:2]:
            raise RuntimeError(
                f"SimCC heads mismatch: {simcc_x.shape} vs {simcc_y.shape}"
            )

        x_idx = np.argmax(simcc_x, axis=-1)
        y_idx = np.argmax(simcc_y, axis=-1)
        x_conf = np.take_along_axis(simcc_x, x_idx[..., None], axis=-1).squeeze(-1)
        y_conf = np.take_along_axis(simcc_y, y_idx[..., None], axis=-1).squeeze(-1)
        width_bins = max(simcc_x.shape[-1] - 1, 1)
        height_bins = max(simcc_y.shape[-1] - 1, 1)
        x_coord = (x_idx / width_bins) * self.input_w
        y_coord = (y_idx / height_bins) * self.input_h
        kp_scores = np.sqrt(np.clip(x_conf * y_conf, 0.0, 1.0))
        boxes = np.stack(
            (
                x_coord.min(axis=1),
                y_coord.min(axis=1),
                x_coord.max(axis=1),
                y_coord.max(axis=1),
            ),
            axis=1,
        )
        pose_scores = kp_scores.mean(axis=1)
        keypoints = np.stack((x_coord, y_coord, kp_scores), axis=-1)
        return boxes, pose_scores, keypoints

    def _detect_full_frame(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        blob, scale, pad = self._prepare_blob(image)
        boxes, scores, keypoints, boxes_are_xywh = self._run_forward(blob)
        return self._build_detections(
            boxes, scores, keypoints, boxes_are_xywh, pad, scale, offset=(0.0, 0.0)
        )

    def _detect_from_person(
        self, image: np.ndarray, bbox: List[float]
    ) -> List[Dict[str, Any]]:
        h, w = image.shape[:2]
        x1, y1, x2, y2 = [int(round(v)) for v in bbox]
        pad_x = int((x2 - x1) * self.person_padding_ratio)
        pad_y = int((y2 - y1) * self.person_padding_ratio)
        x1 = max(0, x1 - pad_x)
        y1 = max(0, y1 - pad_y)
        x2 = min(w, x2 + pad_x)
        y2 = min(h, y2 + pad_y)
        if x2 <= x1 or y2 <= y1:
            return []
        crop = image[y1:y2, x1:x2]
        if crop.size == 0:
            return []
        blob, scale, pad = self._prepare_blob(crop)
        boxes, scores, keypoints, boxes_are_xywh = self._run_forward(blob)
        return self._build_detections(
            boxes,
            scores,
            keypoints,
            boxes_are_xywh,
            pad,
            scale,
            offset=(float(x1), float(y1)),
        )

    def _prepare_blob(
        self, image: np.ndarray
    ) -> Tuple[np.ndarray, float, Tuple[int, int]]:
        img, scale, pad = letterbox(image, (self.input_w, self.input_h))
        blob = img.transpose(2, 0, 1).astype(np.float32) / 255.0
        blob = np.expand_dims(blob, 0)
        return blob, scale, pad

    def _run_forward(
        self, blob: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, bool]:
        outputs = self.session.run(None, {self.input_name: blob})
        if isinstance(outputs, (list, tuple)) and len(outputs) == 2:
            boxes, scores, keypoints = self._decode_simcc(outputs[0], outputs[1])
            boxes_are_xywh = False
        else:
            tensor = outputs[0] if isinstance(outputs, (list, tuple)) else outputs
            tensor = normalise_pose_output(tensor)
            boxes = tensor[:, :4]
            scores = tensor[:, 4]
            keypoints = tensor[:, 5:]
            num_points = keypoints.shape[1] // 3
            keypoints = keypoints.reshape(keypoints.shape[0], num_points, 3)
            boxes_are_xywh = True
        return boxes, scores, keypoints, boxes_are_xywh

    def _build_detections(
        self,
        boxes: np.ndarray,
        scores: np.ndarray,
        keypoints: np.ndarray,
        boxes_are_xywh: bool,
        pad: Tuple[int, int],
        scale: float,
        offset: Tuple[float, float],
    ) -> List[Dict[str, Any]]:
        mask = scores >= self.conf_thres
        boxes, scores, keypoints = boxes[mask], scores[mask], keypoints[mask]
        if boxes.size == 0:
            return []
        if keypoints.ndim != 3 or keypoints.shape[2] != 3:
            num_points = keypoints.shape[1] // 3
            keypoints = keypoints.reshape(keypoints.shape[0], num_points, 3)
        if boxes_are_xywh:
            boxes = xywh_to_xyxy(boxes)
        boxes -= np.array([pad[0], pad[1], pad[0], pad[1]], dtype=np.float32)
        if scale > 0:
            boxes /= scale
            keypoints[:, :, 0] -= pad[0]
            keypoints[:, :, 1] -= pad[1]
            keypoints[:, :, :2] /= scale
        boxes[:, [0, 2]] += offset[0]
        boxes[:, [1, 3]] += offset[1]
        keypoints[:, :, 0] += offset[0]
        keypoints[:, :, 1] += offset[1]
        keep = nms(boxes, scores, self.iou_thres)
        if self.max_det and len(keep) > self.max_det:
            keep = sorted(keep, key=lambda idx: scores[idx], reverse=True)[
                : self.max_det
            ]
        results: List[Dict[str, Any]] = []
        for idx in keep:
            pts = keypoints[idx]
            kpt_list = [(float(x), float(y), float(c)) for x, y, c in pts]
            x1, y1, x2, y2 = boxes[idx]
            results.append(
                {
                    "bbox": [float(x1), float(y1), float(x2), float(y2)],
                    "confidence": float(scores[idx]),
                    "keypoints": kpt_list,
                }
            )
        return results

    def _detect_heuristic(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        h, w = image.shape[:2]
        torso_len = h * 0.25
        center_x = w / 2
        center_y = h / 2
        keypoints = [
            (center_x, center_y - torso_len, 0.8),  # head
            (center_x, center_y - torso_len / 2, 0.9),  # neck
            (center_x, center_y, 0.95),  # spine
            (center_x, center_y + torso_len / 2, 0.9),  # pelvis
        ]
        # Fill to 17 keypoints as expected by yolov11 pose models
        while len(keypoints) < 17:
            angle = (len(keypoints) - 4) * (math.pi / 6)
            radius = torso_len / 1.5
            keypoints.append(
                (
                    center_x + radius * math.cos(angle),
                    center_y + radius * math.sin(angle),
                    0.6,
                )
            )
        bbox = [
            float(center_x - torso_len / 1.2),
            float(center_y - torso_len * 1.2),
            float(center_x + torso_len / 1.2),
            float(center_y + torso_len * 1.2),
        ]
        return [
            {
                "bbox": bbox,
                "confidence": 0.6,
                "keypoints": keypoints,
            }
        ]


class SimulatedDetector(BaseDetector):
    """Fallback detector used for face/action until dedicated pipelines land."""

    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.event_type = config.get("event_type", "object")
        self.interval = int(config.get("interval", 30))
        self._counter = 0

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        self._counter += 1
        if self._counter % self.interval != 0:
            return []
        h, w = image.shape[:2]
        return [
            {
                "bbox": [w * 0.25, h * 0.25, w * 0.75, h * 0.75],
                "confidence": 0.4,
                "class_id": 0,
                "class": self.config.get("simulation_class", "event"),
            }
        ]


# --- Face Detection & Recognition Stubs ---
class FaceDetector(BaseDetector):
    event_type = "face"

    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.input_w = int(config.get("input", {}).get("width", 640))
        self.input_h = int(config.get("input", {}).get("height", 640))
        self.conf_thres = float(config.get("confidence_threshold", 0.5))
        onnx_path = config.get("onnx_path")
        self.session = load_onnx_session(Path(onnx_path)) if onnx_path else None
        self.input_name = (
            self.session.get_inputs()[0].name if self.session else "images"
        )
        self.input_w, self.input_h = _sync_session_dims(
            self.session, self.input_w, self.input_h
        )
        LOGGER.info("Face detector ready; ONNX=%s", bool(self.session))

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        if self.session:
            # TODO: Implement ONNX inference for face detection
            return []
        # Simulate output for now
        h, w = image.shape[:2]
        return [
            {
                "bbox": [w * 0.3, h * 0.3, w * 0.7, h * 0.7],
                "confidence": 0.6,
                "class_id": 0,
                "class": "face",
            }
        ]


class FaceRecognitionDetector(BaseDetector):
    event_type = "face_recognition"

    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)
        self.input_w = int(config.get("input", {}).get("width", 112))
        self.input_h = int(config.get("input", {}).get("height", 112))
        self.conf_thres = float(config.get("confidence_threshold", 0.5))
        onnx_path = config.get("onnx_path")
        self.session = load_onnx_session(Path(onnx_path)) if onnx_path else None
        self.input_name = (
            self.session.get_inputs()[0].name if self.session else "images"
        )
        self.input_w, self.input_h = _sync_session_dims(
            self.session, self.input_w, self.input_h
        )
        LOGGER.info("Face recognition detector ready; ONNX=%s", bool(self.session))

    def detect(self, image: np.ndarray) -> Iterable[Dict[str, Any]]:
        if self.session:
            # TODO: Implement ONNX inference for face recognition
            return []
        # Simulate output for now
        h, w = image.shape[:2]
        return [
            {
                "bbox": [w * 0.3, h * 0.3, w * 0.7, h * 0.7],
                "confidence": 0.7,
                "embedding": [0.0] * 512,
                "class_id": 0,
                "class": "face_recognition",
            }
        ]


def build_detector(config: Dict[str, Any]) -> BaseDetector:
    task = (config.get("task") or "").lower()
    if task in {"object", "object_detection", "detection"}:
        return ObjectDetector(config)
    if task in {"pose", "pose_detection"}:
        return PoseDetector(config)
    if task in {"face", "face_detection"}:
        return FaceDetector(config)
    if task in {"face_recognition", "frs"}:
        return FaceRecognitionDetector(config)
    return SimulatedDetector(config)


__all__ = [
    "BaseDetector",
    "ObjectDetector",
    "PoseDetector",
    "SimulatedDetector",
    "FaceDetector",
    "FaceRecognitionDetector",
    "build_detector",
]
from __future__ import annotations

import os
import time
from collections import deque
from pathlib import Path
from typing import Any, Deque, Dict, List, Optional

import numpy as np
import yaml

from src.models.collapse import CollapseModel
from src.models.gesture import GestureModel
from src.models.phone import PhoneUsage
from src.telemetry.ingest import Telemetry


def _load_yaml(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    try:
        data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    except yaml.YAMLError:
        data = {}
    return data


class PoseUseCaseMonitor:
    """Collects pose-driven heuristics and exposes HUD-friendly status."""

    def __init__(
        self,
        fps_hint: float = 15.0,
        collapse_model: Optional[CollapseModel] = None,
        gesture_model: Optional[GestureModel] = None,
        phone_model: Optional[PhoneUsage] = None,
    ) -> None:
        self.fps_hint = max(fps_hint, 1.0)
        self.collapse = collapse_model or CollapseModel(os.getenv("UC8_COLLAPSE_ONNX"))
        self.gesture = gesture_model or GestureModel()
        self.phone = phone_model or PhoneUsage(fps=int(round(self.fps_hint)))
        self.telemetry = Telemetry()
        self.track_state: Dict[int, Dict[str, Any]] = {}
        self.event_counts = {"collapse": 0, "gesture": 0, "phone": 0}
        self._active_labels: Deque[Dict[str, Any]] = deque()
        self._track_banners: Dict[int, Dict[str, Any]] = {}
        self.banner_ttl = float(os.getenv("POSE_EVENT_BANNER_TTL", "3"))

        collapse_cfg = _load_yaml(Path("configs/usecases/medical_emergency_collapse.demo.yaml"))
        gesture_cfg = _load_yaml(Path("configs/usecases/hand_gesture_signal_identification.demo.yaml"))
        phone_cfg = _load_yaml(Path("configs/usecases/mobile_phone_usage_detection.demo.yaml"))

        thresholds = collapse_cfg.get("thresholds", {})
        self.collapse_threshold = float(thresholds.get("collapse_score", 0.65))
        self.prone_window = int(thresholds.get("followup_prone_s", 4))

        self.gesture_thresholds: Dict[str, float] = {
            label: float(score) for label, score in (gesture_cfg.get("labels") or {}).items()
        }
        self.default_gesture_threshold = float(os.getenv("POSE_GESTURE_THRESHOLD", "0.6"))

        phone_thresholds = phone_cfg.get("thresholds", {})
        self.phone_usage_threshold = float(phone_thresholds.get("phone_usage_score", 0.6))
        gate = phone_cfg.get("gate", {})
        self.phone_min_speed = float(gate.get("min_speed_kmph", 0.0))

    def update_telemetry(self, payload: Optional[Dict[str, Any]]) -> None:
        if not payload:
            return
        if "speed_kmph" in payload:
            self.telemetry.set_speed(payload["speed_kmph"])
        if "door_open" in payload:
            self.telemetry.set_door(payload["door_open"])
        if "gps" in payload:
            self.telemetry.set_gps(payload["gps"])

    def process(self, camera_id: str, detections: List[Dict[str, Any]], fps_hint: float | None = None) -> None:
        fps = fps_hint or self.fps_hint
        for det in detections:
            keypoints = det.get("keypoints")
            track_id = det.get("track_id")
            if keypoints is None or track_id is None:
                continue
            kp_array = np.asarray(keypoints, dtype=np.float32)
            features = self._update_track_state(track_id, kp_array, fps)
            events: List[Dict[str, Any]] = []

            if len(features) >= 3:
                collapse_score = self.collapse.score(features[-15:])
                if collapse_score >= self.collapse_threshold:
                    events.append({"type": "pose.collapse", "score": collapse_score})
                    self._record_event("collapse", f"COLLAPSE {collapse_score:.2f}", track_id)

            label, gesture_score = self.gesture.predict(kp_array)
            min_score = self.gesture_thresholds.get(label, self.default_gesture_threshold)
            if label != "unknown" and gesture_score >= min_score:
                events.append({"type": "pose.gesture", "label": label, "score": gesture_score})
                self._record_event("gesture", label.upper(), track_id)

            phone_score, active = self.phone.score(kp_array)
            if active and phone_score >= self.phone_usage_threshold:
                if self._speed_ok():
                    events.append({"type": "pose.phone_usage", "score": phone_score})
                    self._record_event("phone", "PHONE-USAGE", track_id)

            if events:
                det.setdefault("pose_events", []).extend(events)

    def hud_lines(self) -> List[str]:
        counts = (
            f"Pose events: collapse={self.event_counts['collapse']} "
            f"gesture={self.event_counts['gesture']} phone={self.event_counts['phone']}"
        )
        now = time.time()
        self._active_labels = deque(label for label in self._active_labels if label["expires"] > now)
        banner_lines = [item["label"] for item in self._active_labels]
        return [counts] + banner_lines if banner_lines else [counts]

    def track_labels(self) -> Dict[int, str]:
        now = time.time()
        expired = [track_id for track_id, meta in self._track_banners.items() if meta["expires"] <= now]
        for track_id in expired:
            self._track_banners.pop(track_id, None)
        return {track_id: meta["label"] for track_id, meta in self._track_banners.items()}

    def _speed_ok(self) -> bool:
        snapshot = self.telemetry.snapshot()
        return float(snapshot.get("speed_kmph", 0.0)) >= self.phone_min_speed

    def _update_track_state(self, track_id: int, keypoints: np.ndarray, fps: float) -> List[Dict[str, float]]:
        state = self.track_state.setdefault(track_id, {"history": deque(maxlen=120)})
        midhip = self._midhip(keypoints)
        prev_midhip = state.get("midhip")
        prev_velocity = state.get("velocity", 0.0)
        if prev_midhip is None:
            velocity = 0.0
        else:
            velocity = float(np.linalg.norm(midhip - prev_midhip) * fps)
        accel = velocity - prev_velocity
        prone_height = self._prone_height(keypoints)
        feature = {
            "v_mag": velocity,
            "a_mag": accel,
            "prone_height_px": prone_height,
        }
        history: Deque[Dict[str, float]] = state["history"]
        history.append(feature)
        state["midhip"] = midhip
        state["velocity"] = velocity
        return list(history)

    @staticmethod
    def _midhip(keypoints: np.ndarray) -> np.ndarray:
        try:
            return (keypoints[11][:2] + keypoints[12][:2]) / 2.0
        except Exception:  # pragma: no cover - fallback to zeros
            return np.zeros(2, dtype=np.float32)

    @staticmethod
    def _prone_height(keypoints: np.ndarray) -> float:
        try:
            head_y = float(keypoints[0][1])
            midhip_y = float(((keypoints[11][1] + keypoints[12][1]) / 2.0))
            return abs(head_y - midhip_y) * 2.0
        except Exception:  # pragma: no cover - fallback height
            return 400.0

    def _record_event(self, kind: str, label: str, track_id: Optional[int] = None) -> None:
        self.event_counts[kind] += 1
        self._active_labels.append({"label": label, "expires": time.time() + self.banner_ttl})
        if track_id is not None:
            self._track_banners[track_id] = {"label": label, "expires": time.time() + self.banner_ttl}


__all__ = ["PoseUseCaseMonitor"]
from __future__ import annotations

import json
import logging
import os
import time
from pathlib import Path
from typing import Dict, Iterable, List, Optional

import cv2
import numpy as np
import yaml

from src.trackers import TrackerManager

from src.common.event_bus import Event, FileEventBus
from .detectors import BaseDetector, build_detector
from .pose_assoc import associate_pose_tracks
from .renderer import draw_hud, draw_pose, draw_track_label
from .pose_events import PoseUseCaseMonitor

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("detector-runner")

CACHE_DIR = Path("artifacts/detections/cache")
FRAMES_LOG = Path(os.getenv("INGEST_FRAMES_LOG", "artifacts/ingest/frames.log"))
SHOW_PREVIEW = os.getenv("LIVE_DEMO_SHOW", "").lower() in {"1", "true", "yes"}
RECORD_PATH = os.getenv("LIVE_DEMO_RECORD")
RECORD_FPS = float(os.getenv("LIVE_DEMO_RECORD_FPS", os.getenv("LIVE_DEMO_CAPTURE_FPS", "25")))
PREVIEW_WINDOW = os.getenv("LIVE_DEMO_WINDOW_NAME", "Aixavier Pose Demo")
OBJECT_TRACKERS = TrackerManager(
    algorithm=os.getenv("TRACKER_ALGO", "bytetrack"),
    high_thresh=float(os.getenv("TRACKER_HIGH_THRESH", "0.6")),
    low_thresh=float(os.getenv("TRACKER_LOW_THRESH", "0.1")),
    match_iou=float(os.getenv("TRACKER_MATCH_IOU", "0.3")),
    max_age=int(os.getenv("TRACKER_MAX_AGE", "30")),
)
POSE_TRACKERS = TrackerManager(
    algorithm=os.getenv("POSE_TRACKER_ALGO", "simple"),
    high_thresh=float(os.getenv("POSE_TRACKER_HIGH_THRESH", "0.4")),
    low_thresh=float(os.getenv("POSE_TRACKER_LOW_THRESH", "0.1")),
    match_iou=float(os.getenv("POSE_TRACKER_MATCH_IOU", "0.2")),
    max_age=int(os.getenv("POSE_TRACKER_MAX_AGE", "30")),
)


def stream_frames(path: Path) -> Iterable[Dict[str, object]]:
    offset = 0
    while True:
        if not path.exists():
            time.sleep(1)
            continue
        with path.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            while True:
                line = fh.readline()
                if not line:
                    break
                offset = fh.tell()
                if not line.strip():
                    continue
                try:
                    yield json.loads(line)
                except json.JSONDecodeError:
                    LOGGER.warning("Skipping malformed frame entry: %s", line)
        time.sleep(0.1)


def load_config() -> Dict[str, object]:
    cfg_path = os.environ.get("DETECTOR_CONFIG")
    if not cfg_path:
        raise RuntimeError("DETECTOR_CONFIG environment variable is required.")
    path = Path(cfg_path)
    if not path.exists():
        raise FileNotFoundError(f"Detector config {path} not found")
    with path.open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh) or {}


def build_event_payload(detector: BaseDetector, frame: Dict[str, object], detection: Dict[str, object], idx: int) -> Dict[str, object]:
    camera_id = frame.get("camera_id", "CAM01")
    timestamp = frame.get("timestamp", time.time())
    frame_index = frame.get("frame_index", idx)
    payload = {
        "camera_id": camera_id,
        "timestamp": timestamp,
        "frame_index": frame_index,
    }
    payload.update(detection)
    payload.setdefault("first_seen", detection.get("first_seen", timestamp))
    track_id = payload.get("track_id")
    if track_id is None:
        bbox = detection.get("bbox")
        bbox_key = tuple(bbox) if isinstance(bbox, (list, tuple)) else bbox
        payload["track_id"] = hash((camera_id, frame_index, bbox_key)) % 10_000
    else:
        payload["track_id"] = int(track_id)
    return payload


def update_object_cache(camera_id: str, timestamp: float, detections: List[Dict[str, object]]) -> None:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    path = CACHE_DIR / f"{camera_id}.json"
    temp = path.with_suffix(".tmp")
    payload = {
        "timestamp": timestamp,
        "detections": detections,
    }
    try:
        temp.write_text(json.dumps(payload), encoding="utf-8")
        temp.replace(path)
    except OSError as exc:  # pragma: no cover - filesystem edge
        LOGGER.warning("Failed to update object cache for %s: %s", camera_id, exc)


def load_object_cache(camera_id: str) -> List[Dict[str, object]]:
    path = CACHE_DIR / f"{camera_id}.json"
    if not path.exists():
        return []
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        return data.get("detections", [])
    except (OSError, json.JSONDecodeError):  # pragma: no cover - corrupted cache
        return []


def assign_pose_ids(camera_id: str, detections: List[Dict[str, object]]) -> List[Dict[str, object]]:
    persons = [
        det
        for det in OBJECT_TRACKERS.latest_detections(camera_id)
        if (det.get("class") or "").lower() == "person"
    ]
    return associate_pose_tracks(persons, detections)


def cleanup_frame(path: str | Path | None, persist: bool) -> None:
    if persist or not path:
        return
    try:
        Path(path).unlink()
    except FileNotFoundError:
        return
    except OSError as exc:
        LOGGER.debug("Failed to delete frame %s: %s", path, exc)


def run() -> None:
    config = load_config()
    detector = build_detector(config)
    pose_monitor: Optional[PoseUseCaseMonitor] = None
    if detector.event_type == "pose":
        fps_hint = float(os.getenv("LIVE_DEMO_CAPTURE_FPS", os.getenv("LIVE_DEMO_RECORD_FPS", "25")))
        pose_monitor = PoseUseCaseMonitor(fps_hint=fps_hint)

    publish_path = config.get("publish_path", "artifacts/detections/events.log")
    publish_dir = Path(publish_path)
    bus = FileEventBus(publish_dir.parent, filename=publish_dir.name)
    log_path = FRAMES_LOG
    interval = int(config.get("interval", 1))
    frame_skip = 0
    render_enabled = SHOW_PREVIEW or bool(RECORD_PATH)
    writer: Optional[cv2.VideoWriter] = None

    LOGGER.info("Starting detector %s; publishing to %s", detector.__class__.__name__, publish_path)

    try:
        for frame in stream_frames(log_path):
            persist_frame = bool(frame.get("persist", True))
            frame_path = frame.get("path")
            if pose_monitor is not None:
                pose_monitor.update_telemetry(frame.get("telemetry"))
            frame_skip = (frame_skip + 1) % interval
            if frame_skip:
                cleanup_frame(frame_path, persist_frame)
                continue
            if not frame_path:
                LOGGER.warning("Frame path missing; skipping frame.")
                continue
            path_obj = Path(frame_path)
            if not path_obj.exists():
                LOGGER.warning("Frame path %s missing; skipping frame.", frame_path)
                cleanup_frame(frame_path, persist_frame)
                continue
            image = cv2.imread(str(path_obj))
            if image is None:
                LOGGER.warning("Failed to read frame %s; skipping.", frame_path)
                cleanup_frame(frame_path, persist_frame)
                continue

            start = time.time()
            timestamp = float(frame.get("timestamp", time.time()))
            camera_id = frame.get("camera_id", "CAM01")
            detections = list(detector.detect(image))
            proc_time = time.time() - start
            fps_est = 1.0 / proc_time if proc_time > 0 else 0.0
            latency_ms = proc_time * 1000.0
            track_labels: Dict[int, str] = {}

            for det in detections:
                det.setdefault("first_seen", timestamp)
                det.setdefault("timestamp", timestamp)
                det.setdefault("latency_ms", latency_ms)
                det.setdefault("detector", detector.config.get("model", detector.__class__.__name__))

            if detector.event_type == "object":
                detections = OBJECT_TRACKERS.update(camera_id, detections)
                update_object_cache(camera_id, timestamp, detections)
            elif detector.event_type == "pose":
                detections = assign_pose_ids(camera_id, detections)
                detections = POSE_TRACKERS.update(camera_id, detections)
                if pose_monitor is not None:
                    pose_monitor.process(camera_id, detections, fps_est or pose_monitor.fps_hint)
                    track_labels = pose_monitor.track_labels()

            vis_frame = None
            if render_enabled:
                vis_frame = image.copy()
                if detector.event_type == "pose":
                    for det in detections:
                        keypoints = det.get("keypoints")
                        if not keypoints:
                            continue
                        vis_frame = draw_pose(
                            vis_frame,
                            np.asarray(keypoints, dtype=np.float32),
                            det.get("bbox"),
                        )
                        if pose_monitor is not None:
                            track_label = track_labels.get(det.get("track_id"))
                            if track_label:
                                vis_frame = draw_track_label(vis_frame, det.get("bbox"), track_label)
                hud_lines = [
                    f"FPS={fps_est:.1f} frame={frame.get('frame_index', '-')}",
                    f"detections={len(detections)} camera={camera_id}",
                ]
                if pose_monitor is not None and detector.event_type == "pose":
                    hud_lines.extend(pose_monitor.hud_lines())
                vis_frame = draw_hud(vis_frame, hud_lines)

            for idx, detection in enumerate(detections):
                payload = build_event_payload(detector, frame, detection, idx)
                payload.setdefault("latency_ms", latency_ms)
                payload.setdefault("detector", detector.config.get("model", detector.__class__.__name__))
                bus.publish(Event(type=detector.event_type, payload=payload))

            if SHOW_PREVIEW and vis_frame is not None:
                cv2.imshow(PREVIEW_WINDOW, vis_frame)
                if cv2.waitKey(1) & 0xFF == 27:
                    raise KeyboardInterrupt

            if RECORD_PATH and vis_frame is not None:
                if writer is None:
                    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
                    writer = cv2.VideoWriter(
                        RECORD_PATH,
                        fourcc,
                        RECORD_FPS,
                        (vis_frame.shape[1], vis_frame.shape[0]),
                    )
                    if not writer.isOpened():
                        LOGGER.warning("Failed to open video writer at %s", RECORD_PATH)
                        writer = None
                if writer is not None:
                    writer.write(vis_frame)

            cleanup_frame(frame_path, persist_frame)
    finally:
        if writer is not None:
            writer.release()
        if SHOW_PREVIEW:
            cv2.destroyWindow(PREVIEW_WINDOW)


def main() -> None:
    try:
        run()
    except KeyboardInterrupt:
        LOGGER.info("Detector interrupted; shutting down.")


if __name__ == "__main__":
    main()
import threading
from typing import Any, Dict


class Telemetry:
    """Thread-safe telemetry snapshot for demo workloads."""

    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._snapshot: Dict[str, Any] = {
            "speed_kmph": 0.0,
            "door_open": False,
            "gps": None,
        }

    def snapshot(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._snapshot)

    def set_speed(self, value: float) -> None:
        with self._lock:
            self._snapshot["speed_kmph"] = float(value)

    def set_door(self, open_: bool) -> None:
        with self._lock:
            self._snapshot["door_open"] = bool(open_)

    def set_gps(self, gps: Any) -> None:
        with self._lock:
            self._snapshot["gps"] = gps
"""Telemetry helpers for demo pipelines."""
"""Package init."""
from __future__ import annotations

import json
import logging
import time
from pathlib import Path

from common.event_bus import Event, FileEventBus

logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")
LOGGER = logging.getLogger("events")


def stream_events(path: Path):
    offset = 0
    while True:
        if not path.exists():
            time.sleep(1)
            continue
        with path.open("r", encoding="utf-8") as fh:
            fh.seek(offset)
            for line in fh:
                offset = fh.tell()
                if not line.strip():
                    continue
                yield json.loads(line)
        time.sleep(0.2)


def main() -> None:
    source = Path("artifacts/events/events.log")
    sink = FileEventBus(Path("artifacts/normalized"))
    for event in stream_events(source):
        normalized = Event(
            type=event.get("type", "unknown"),
            payload={
                "camera_id": event.get("camera_id"),
                "ts": event.get("timestamp", time.time()),
                "attributes": event,
            },
        )
        LOGGER.info("Normalized event %s", normalized.type)
        sink.publish(normalized)


if __name__ == "__main__":
    main()
"""
Application entrypoints exposed via ``python -m src.apps.<name>``.

Keep modules here as thin shims over the real runners so CLI UX remains
stable even as internal package layout evolves.
"""

"""
Launch a self-contained live demo by orchestrating ingest + detector.

Usage:
    python -m src.apps.live_demo --config configs/detectors/pose_velocity.yaml --source webcam:0
"""

from __future__ import annotations

import argparse
import os
import signal
import subprocess
import sys
import time
from pathlib import Path
from typing import Sequence, Tuple


DEFAULT_CONFIG = "configs/detectors/pose_velocity.yaml"
DEFAULT_OUTPUT = "artifacts/live_demo"


def parse_args(argv: Sequence[str] | None = None) -> Tuple[argparse.Namespace, list[str]]:
    parser = argparse.ArgumentParser(description="Run a detector locally with an auto-launched ingest loop.")
    parser.add_argument(
        "--config",
        default=DEFAULT_CONFIG,
        help="Detector config YAML; exported as DETECTOR_CONFIG (default: %(default)s).",
    )
    parser.add_argument(
        "--source",
        default="webcam:0",
        help="Input source hint (webcam:N, demo://synthetic, rtsp://, file path).",
    )
    parser.add_argument(
        "--output",
        default=DEFAULT_OUTPUT,
        help="Base directory for live demo artifacts (frames, logs).",
    )
    parser.add_argument(
        "--camera-id",
        default="CAM_WEB",
        help="Camera ID recorded in emitted events.",
    )
    parser.add_argument(
        "--camera-name",
        default="Live Demo Camera",
        help="Human-friendly camera label stored in the temporary config.",
    )
    parser.add_argument(
        "--fps",
        type=int,
        default=25,
        help="Capture FPS limit passed to ingest.",
    )
    parser.add_argument(
        "--save-frames",
        action="store_true",
        help="Keep captured frames on disk (default: delete after detector consumes them).",
    )
    parser.add_argument(
        "--show",
        action="store_true",
        help="Open a live preview window with pose overlays.",
    )
    parser.add_argument(
        "--record",
        default="",
        help="Path to save annotated MP4 (requires OpenCV VideoWriter).",
    )
    return parser.parse_known_args(argv)


def normalize_source(source: str) -> str:
    if source.startswith("webcam://"):
        return source
    if source.startswith("webcam:"):
        return f"webcam://{source.split(':', 1)[1]}"
    return source


def reset_frames_log(log_path: Path) -> None:
    log_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        log_path.unlink()
    except FileNotFoundError:
        pass


def launch_ingest(
    source: str,
    output_dir: Path,
    frames_log: Path,
    *,
    camera_id: str,
    camera_name: str,
    fps: int,
    save_frames: bool,
    env: dict[str, str],
) -> subprocess.Popen:
    cmd = [
        sys.executable,
        "-m",
        "src.ingest_gst.main",
        "--source",
        source,
        "--camera-id",
        camera_id,
        "--camera-name",
        camera_name,
        "--fps",
        str(fps),
        "--output",
        str(output_dir),
        "--log-path",
        str(frames_log),
    ]
    if save_frames:
        cmd.append("--save-frames")
    proc = subprocess.Popen(cmd, env=env)
    # Give the subprocess a moment to fail fast if configuration is invalid.
    time.sleep(0.5)
    if proc.poll() is not None:
        raise RuntimeError(f"Ingest process exited immediately with code {proc.returncode}.")
    return proc


def terminate_process(proc: subprocess.Popen | None, name: str) -> None:
    if not proc:
        return
    if proc.poll() is not None:
        return
    proc.send_signal(signal.SIGINT)
    try:
        proc.wait(timeout=5)
    except subprocess.TimeoutExpired:
        proc.terminate()
        try:
            proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            proc.kill()


def main(argv: Sequence[str] | None = None) -> int:
    args, passthrough = parse_args(argv)
    env = os.environ.copy()
    detector_config = Path(args.config).resolve()
    output_dir = Path(args.output).resolve()
    ingest_dir = output_dir / "ingest"
    frames_log = ingest_dir / "frames.log"
    reset_frames_log(frames_log)

    normalized_source = normalize_source(args.source)
    env["DETECTOR_CONFIG"] = str(detector_config)
    env["LIVE_DEMO_SOURCE"] = normalized_source
    env["INGEST_OUTPUT"] = str(ingest_dir)
    env["INGEST_FRAMES_LOG"] = str(frames_log)
    if args.show:
        env["LIVE_DEMO_SHOW"] = "1"
    if args.record:
        env["LIVE_DEMO_RECORD"] = str(Path(args.record).resolve())
        env["LIVE_DEMO_RECORD_FPS"] = str(args.fps)

    ingest_proc: subprocess.Popen | None = None
    return_code = 1
    try:
        ingest_proc = launch_ingest(
            normalized_source,
            ingest_dir,
            frames_log,
            camera_id=args.camera_id,
            camera_name=args.camera_name,
            fps=args.fps,
            save_frames=args.save_frames,
            env=env,
        )
        cmd = [sys.executable, "-m", "src.runners.main", *passthrough]
        return_code = subprocess.call(cmd, env=env)
    except KeyboardInterrupt:
        return_code = 130
    finally:
        terminate_process(ingest_proc, "ingest")
    return return_code


if __name__ == "__main__":
    raise SystemExit(main())
"""Synthetic loop emitting demo events for pose-based use cases."""

from __future__ import annotations

import os
import random
import time
from pathlib import Path

from src.common.event_bus import Event, FileEventBus
from src.models.collapse import CollapseModel
from src.models.gesture import GestureModel
from src.models.phone import PhoneUsage
from src.telemetry.ingest import Telemetry


def _fake_pose_sequence(length: int = 180, width: int = 640, height: int = 360):
    keypoints = [[width * 0.5, height * 0.5] for _ in range(17)]
    for frame in range(length):
        for idx in range(17):
            keypoints[idx][0] += random.uniform(-0.6, 0.6)
            keypoints[idx][1] += random.uniform(-0.6, 0.6)
        if 40 <= frame <= 50:
            keypoints[11][1] += 8.0
            keypoints[12][1] += 8.0
        if 51 <= frame <= 70:
            for idx in (0, 1, 2, 3, 4, 5, 6, 11, 12):
                keypoints[idx][1] = min(height - 5, keypoints[idx][1] + 4.5)
        yield [[float(x), float(y)] for x, y in keypoints]


def run_demo(out_dir: Path, fps: int = 15) -> None:
    bus = FileEventBus(out_dir)
    telemetry = Telemetry()
    collapse_model = CollapseModel(onnx_path=None)
    gesture_model = GestureModel()
    phone_model = PhoneUsage(fps=fps)

    feature_ring: list[dict[str, float]] = []
    previous_midhip: tuple[float, float] | None = None

    for keypoints in _fake_pose_sequence():
        midhip = (
            (keypoints[11][0] + keypoints[12][0]) / 2.0,
            (keypoints[11][1] + keypoints[12][1]) / 2.0,
        )
        if previous_midhip is None:
            velocity = 0.0
            accel = 0.0
        else:
            velocity = (
                (midhip[0] - previous_midhip[0]) ** 2
                + (midhip[1] - previous_midhip[1]) ** 2
            ) ** 0.5 * fps
            accel = velocity - feature_ring[-1]["v_mag"] if feature_ring else 0.0
        previous_midhip = midhip
        bbox_height = (
            max(
                1.0,
                abs(keypoints[0][1] - ((keypoints[11][1] + keypoints[12][1]) / 2.0)),
            )
            * 2.0
        )
        features = {"v_mag": velocity, "a_mag": accel, "prone_height_px": bbox_height}
        feature_ring.append(features)
        if len(feature_ring) > 90:
            feature_ring.pop(0)

        collapse_score = (
            collapse_model.score(feature_ring[-15:]) if len(feature_ring) >= 15 else 0.0
        )
        if collapse_score >= 0.65:
            bus.publish(
                Event(
                    "pose.collapse", {"score": collapse_score, "prone_px": bbox_height}
                )
            )

        label, gesture_score = gesture_model.predict(keypoints)
        if gesture_score >= 0.6 and label != "unknown":
            bus.publish(Event("pose.gesture", {"label": label, "score": gesture_score}))

        phone_score, active = phone_model.score(keypoints)
        if active:
            bus.publish(Event("pose.phone_usage", {"score": phone_score}))

        telemetry.set_speed(random.uniform(0, 15))
        time.sleep(1.0 / fps)


def main() -> int:
    out_dir = Path(os.environ.get("AIX_ARTIFACTS", "./artifacts")).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)
    run_demo(out_dir=out_dir)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
"""
Stub for frontend UI integration (to be replaced with actual frontend code).
"""

# This file is a placeholder for future frontend integration.
# Add FastAPI static file serving or frontend build hooks here.
# UI module for ROI editor, event browser, and toggles
"""
FastAPI router aggregator for UI endpoints.
"""

from fastapi import APIRouter
from .roi_editor import router as roi_router
from .event_browser import router as event_router
from .toggles import router as toggles_router

router = APIRouter()
router.include_router(roi_router)
router.include_router(event_router)
router.include_router(toggles_router)
"""
Event Browser API and stub implementation.
"""

from fastapi import APIRouter
from pydantic import BaseModel
from typing import List

router = APIRouter()


class Event(BaseModel):
    id: str
    timestamp: str
    type: str
    details: str


# In-memory store for demo
_event_store: List[Event] = []


@router.get("/ui/events", response_model=List[Event])
def list_events():
    return _event_store


@router.post("/ui/events", response_model=Event)
def add_event(event: Event):
    _event_store.append(event)
    return event
"""
UI Toggles API and stub implementation.
"""

from fastapi import APIRouter
from pydantic import BaseModel
from typing import Dict

router = APIRouter()


class Toggle(BaseModel):
    name: str
    enabled: bool


# In-memory store for demo
_toggle_store: Dict[str, bool] = {}


@router.get("/ui/toggles", response_model=Dict[str, bool])
def list_toggles():
    return _toggle_store


@router.post("/ui/toggles", response_model=Toggle)
def set_toggle(toggle: Toggle):
    _toggle_store[toggle.name] = toggle.enabled
    return toggle
"""
ROI Editor API and stub implementation.
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Tuple

router = APIRouter()


class ROI(BaseModel):
    id: str
    points: List[Tuple[int, int]]
    label: str


# In-memory store for demo
roi_store: List[ROI] = []


@router.get("/ui/roi", response_model=List[ROI])
def list_rois():
    return roi_store


@router.post("/ui/roi", response_model=ROI)
def add_roi(roi: ROI):
    roi_store.append(roi)
    return roi


@router.delete("/ui/roi/{roi_id}")
def delete_roi(roi_id: str):
    global roi_store
    roi_store = [r for r in roi_store if r.id != roi_id]
    return {"status": "deleted"}
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional

import yaml


@dataclass(frozen=True)
class UseCaseDefinition:
    """Canonical metadata for a railway analytics use case."""

    id: int
    slug: str
    name: str
    description: str
    applicability: str
    deployment: str
    detectors: List[str]
    sensors: List[str]
    maturity: str

    def to_metadata(self) -> Dict[str, str]:
        return {
            "id": self.slug,
            "name": self.name,
            "description": self.description,
            "applicability": self.applicability,
            "deployment": self.deployment,
            "detectors": ", ".join(self.detectors),
            "sensors": ", ".join(self.sensors),
            "maturity": self.maturity,
        }


class UseCaseRegistry:
    """Loads the spreadsheet-backed manifest and exposes helper queries."""

    def __init__(
        self,
        manifest_path: Path | str = Path("assets/usecases/catalog.yaml"),
        configs_dir: Path | str = Path("configs/usecases"),
    ) -> None:
        self.manifest_path = Path(manifest_path)
        self.configs_dir = Path(configs_dir)
        self._definitions = self._load_manifest()
        self._by_slug = {definition.slug: definition for definition in self._definitions}

    def _load_manifest(self) -> List[UseCaseDefinition]:
        if not self.manifest_path.exists():
            raise FileNotFoundError(f"Use case manifest missing: {self.manifest_path}")
        payload = yaml.safe_load(self.manifest_path.read_text(encoding="utf-8")) or {}
        rows = payload.get("usecases", [])
        definitions: List[UseCaseDefinition] = []
        for row in rows:
            definitions.append(UseCaseDefinition(**row))
        return definitions

    @property
    def definitions(self) -> List[UseCaseDefinition]:
        return list(self._definitions)

    def get(self, slug: str) -> Optional[UseCaseDefinition]:
        return self._by_slug.get(slug)

    def missing_configs(self) -> List[str]:
        """Return slugs that do not yet have a YAML rule definition."""
        missing: List[str] = []
        for definition in self._definitions:
            if not (self.configs_dir / f"{definition.slug}.yaml").exists():
                missing.append(definition.slug)
        return missing

    def maturity_counts(self) -> Dict[str, int]:
        counts: Dict[str, int] = {}
        for definition in self._definitions:
            counts[definition.maturity] = counts.get(definition.maturity, 0) + 1
        return counts

    def enrich_metadata(self, slug: str, metadata: Optional[Dict[str, str]]) -> Dict[str, str]:
        """Merge manifest metadata with a config-specific metadata block."""
        enriched: Dict[str, str] = {}
        definition = self.get(slug)
        if definition:
            enriched.update(definition.to_metadata())
        if metadata:
            enriched.update(metadata)
        if "id" not in enriched:
            enriched["id"] = slug
        return enriched

    def to_report(self) -> Dict[str, object]:
        return {
            "total": len(self._definitions),
            "maturity": self.maturity_counts(),
            "missing_configs": self.missing_configs(),
        }


__all__ = ["UseCaseDefinition", "UseCaseRegistry"]
"""Core coordination utilities for agents and orchestration."""

from .usecases import UseCaseDefinition, UseCaseRegistry

__all__ = ["UseCaseDefinition", "UseCaseRegistry"]
"""Aixavier namespace package for new runtime/lib modules."""

from importlib import import_module as _import_module

__all__ = ["core"]
# Aixavier Edge Analytics Stack

> Railway-grade CCTV analytics, automation agents, and orchestration tooling tuned for NVIDIA Jetson Xavier (AGX / NX).

This repository contains everything needed to ingest RTSP/ONVIF feeds, execute 22 analytics workloads, surface events, and keep the stack documented through an automation agent that curates model intel and changelogs. The current layout is being refactored toward the conventions captured in `AGENTS.md`; refer to that guide when adding new modules under `src/aixavier/`.

## Quickstart
```bash
git clone https://github.com/{{ORG_NAME}}/aixavier.git
cd aixavier
cp .env.example .env                    # keep placeholders until secrets are issued
make placeholders:list                  # discover required variables
make placeholders:resolve FROM=.env     # map .env into configs/*
make bootstrap                          # create .venv, install deps, warm TensorRT engines
make demo                               # bring up docker-compose demo with synthetic RTSP feed
```

Once the demo profile is healthy, switch to production profiles by exporting `PROFILE=all` (full 22-rule workload) or pointing to a bespoke profile file via `PROFILE=path/to/profile.yaml`.

**Important:** Run `make placeholders:check` before shipping or deploying. Only commit `.env.example` and templates—never real secrets. See [`docs/placeholders.md`](docs/placeholders.md) for the live inventory.

## Daily Driver Commands
| Command                                   | Purpose                                                                                                                 |
| ----------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| `make bootstrap`                          | Bootstrap virtualenv, install runtime + dev tooling, pre-build engines.                                                 |
| `make run PROFILE=<name>`                 | Launch the docker compose stack for the selected profile.                                                               |
| `make demo`                               | Generate demo assets and start the curated eight-rule showcase.                                                         |
| `make perf`                               | Capture FPS, latency, and GPU utilisation snapshots from the exporter API.                                              |
| `make test`                               | Run pytest suite plus RTSP smoke tests.                                                                                 |
| `make lint`                               | Run ruff (lint) and mypy (type checks) over `src` and `tools`.                                                          |
| `make agent:refresh -- --dry-run`         | Exercise the maintainer agent without mutating docs.                                                                    |
| `make clean`                              | Tear down compose services, remove the virtualenv, and clear caches.                                                    |
| `make live CONFIG=... SRC=...`            | Run the local detector pipeline; spawns ingest + detector with your config/source (defaults to pose velocity + webcam). |
| `SHOW=1 make live ...`                    | Same as above, but opens a preview window with pose overlays.                                                           |
| `RECORD=artifacts/pose.mp4 make live ...` | Same as above, but records the annotated frames to MP4 (combine with `SHOW=1` if desired).                              |

### Model assets
- Place development ONNX exports under `models/object/onnx/` and `models/pose/onnx/` if you want CPU-backed inference.
- Promote production TensorRT engines (FP16/INT8) into `models/usecases/<use-case>/fp16|int8/` and update `configs/detectors/*.yaml` to match.
- Run `models/bootstrap_models.py` to generate placeholder files when CI needs to stub missing assets.

### Multi-person pose (top-down)
- `configs/detectors/pose_velocity.yaml` now includes a `person_detector` block (default: YOLO11n) so the pose runner crops **each person** before running RTMPose. Populate `models/object/onnx/yolo11n.onnx` (or any compatible detector), adjust the thresholds if needed, and run `SHOW=1 make live`—each person receives an independent skeleton and banner.
- Exporting YOLO11n to ONNX:
  ```bash
  source .venv/bin/activate
  pip install --upgrade ultralytics
  yolo export model=yolo11n.pt format=onnx imgsz=640 dynamic=False simplify=True
  mv yolo11n.onnx models/object/onnx/
  ```
- If you prefer bottom-up pose (single-person), remove the `person_detector` section and the runtime will fall back to whole-frame inference.

### Tracker configuration
- ByteTrack-lite is enabled by default (`TRACKER_ALGO=bytetrack`). Override with `TRACKER_ALGO=simple` if you need the lightweight IoU tracker for debugging.
- Pose trackers use `POSE_TRACKER_ALGO` (defaults to `simple`); set to `bytetrack` once you need tighter pose ID smoothing.
- Additional knobs: `TRACKER_HIGH_THRESH`, `TRACKER_LOW_THRESH`, `TRACKER_MATCH_IOU`, `TRACKER_MAX_AGE` (and matching `POSE_TRACKER_*` variables).

### Webcam smoke test
For a one-shot local run that wires up ingest + detectors automatically:
```bash
make live SRC=webcam:0 CONFIG=configs/detectors/pose_velocity.yaml
```
Behind the scenes this command spawns `src.ingest_gst.main` with your chosen source (webcam, RTSP, file, or synthetic) and feeds frames to `src.runners.main`. Artifacts land under `artifacts/live_demo/` so you can inspect captured frames and emitted events. Override `OUTPUT`, `CAMERA`, or `FPS` when you need custom layouts.

If you prefer using the docker-compose stack, you can still export `CAMERA_RTSP_URL_01=webcam://0` and run `make run PROFILE=demo` to test the full pipeline.

Want a quick visual? Add `--show` (or `SHOW=1 make live ...`) to open an OpenCV window with skeleton overlays, or `--record artifacts/pose_demo.mp4` (or `RECORD=... make live ...`) to capture an annotated MP4 for sharing.

To drive ingest manually (e.g., when you only want to capture frames/logs), use the new CLI flags:
```bash
python -m src.ingest_gst.main \
  --source webcam:0 \
  --output artifacts/ingest \
  --log-path artifacts/ingest/frames.log \
  --camera-id CAM_WEB
```
By default frames are ephemeral—the detector deletes them right after reading so your disk doesn’t fill up. Pass `--save-frames` if you genuinely need every JPEG persisted for later inspection. `--source` accepts `webcam:<index>`, `demo://synthetic`, RTSP URLs, or file paths; omit it to read from `configs/cameras.yaml` as before.

Want persistent captures during `make live`? Just run `SAVE_FRAMES=1 make live ...`.

### Generating detector engines
1. Export ONNX (if you haven’t already) and place it under `models/usecases/<use-case>/onnx/`.
2. Convert to TensorRT FP16 on Jetson/x86 with TensorRT installed:
   ```bash
   trtexec \
     --onnx=models/usecases/pose/rtmpose_onnx/.../end2end.onnx \
     --saveEngine=models/usecases/pose/fp16/rtmpose.engine \
     --fp16 --workspace=4096
   trtexec \
     --onnx=models/usecases/object_detection/onnx/yolov11n.onnx \
     --saveEngine=models/usecases/object_detection/fp16/yolov11n.engine \
     --fp16 --workspace=4096
   ```
3. Update `configs/detectors/object.yaml` / `configs/detectors/pose_velocity.yaml` if your filenames differ. Leave INT8 calibration for later (the scripts under `models/*/calibrate.sh` act as placeholders until we capture calibration sets).

## Runtime Profiles at a Glance
| Profile   | Focus                          | FPS Target | Notable Intervals                                          |
| --------- | ------------------------------ | ---------- | ---------------------------------------------------------- |
| `minimal` | Tamper + Trespass              | 30         | Detectors on every frame.                                  |
| `demo`    | Eight representative use cases | 25         | Action detectors every 3 frames, face recognition every 2. |
| `all`     | Full 22-rule workload          | 25         | Per-use-case intervals in `configs/usecases/*.yaml`.       |

Select a profile by exporting `PROFILE=<name>` or passing `PROFILE=...` to individual make targets. Compose profiles live under `docker-compose.yml`.

## System Overview
1. **Ingest (`src/ingest_gst/`)** – DeepStream-based RTSP/ONVIF ingestion with automatic reconnects, watermarks, and NVMM zero-copy paths.
2. **Runners (`src/runners/`)** – TensorRT engines (FP16 baseline, INT8 optional) grouped by modality: object, face, pose, action, smoke, and violence. The shared detector runtime (`src/runners/detectors.py`) can fall back to ONNX/CPU inference when engines are absent, so you can validate pipelines before dropping production engines into `models/usecases/`.
3. **Trackers (`src/trackers/`)** – ByteTrack-lite with optional fallback to `SimpleTracker`; pose IDs are associated to the latest person tracks. Wire in OSNet ReID for production deployments.
4. **Rules (`src/rules/`)** – YAML-driven behavioural engine handling ROIs, dwell times, line crossings, and domain-specific semantics.
5. **Privacy (`src/privacy/`)** – SCRFD + ArcFace inference, encrypted embeddings, RBAC checks, and audit logging.
6. **Events (`src/events/`)** – Normalises detections, ships MQTT/REST payloads, and persists artefacts.
7. **Recorder (`src/recorder/`)** – Circular buffers with pre/post recording windows, watermarking, hashing, and export flows.
8. **Exporter (`src/exporter/`)** – Prometheus metrics and OpenTelemetry traces surfaced via HTTP.
9. **UI (`src/aixavier/ui/`)** – FastAPI backend exposes endpoints for ROI editing, event browser, and toggles. See `src/aixavier/ui/README.md` for API details. Frontend integration is planned.
10. **Automation Agent (`src/agent/`)** – Gathers model intelligence, updates documentation, and logs activity to `HANDOFF.md`.

The refactor toward `src/aixavier/` will co-locate agents under `src/aixavier/agents/` and shared infrastructure under `src/aixavier/core/`. Existing modules will migrate in-place; prefer those paths for new code.

## Repository Layout
- `AGENTS.md` – Contribution, testing, and naming conventions (read before opening PRs).
- `configs/` – Profiles, rulepacks, camera definitions, and placeholder-friendly YAML.
- `deploy/` – Flash scripts, systemd units, Grafana dashboards, and logrotate configs.
- `docs/` – Operational runbooks (`SETUP.md`, `PERFORMANCE.md`, `PRIVACY.md`, `TROUBLESHOOT.md`, etc).
- `docs/IMPLEMENTATION_STATUS.md` – Snapshot of current service/model progress for quick onboarding.
- `models/` – TensorRT engine builders and calibration scripts.
- `models/usecases/` – drop-in folder for the 22 production-ready detection/pose engines (place FP16/INT8 artifacts here when promoting models).
- `src/` – Runtime services and automation agent (pending relocation to `src/aixavier/`).
- `tests/` – Pytest suites, fixtures, and RTSP connectivity scripts.
- `tools/` – Placeholder resolvers, lint helpers, and release automation.
- `HANDOFF.md` – Append-only log shared with the automation agent.
- `.env.example` – Placeholder inventory; keep secrets out of Git.

## Documentation & Observability
- Metrics surface at `http://{{PROMETHEUS_SCRAPE_PORT}}/metrics`; Grafana dashboard JSON lives in `deploy/grafana/edge-cctv.json`.
- Refer to `docs/TROUBLESHOOT.md` for RTSP/ONVIF, codec, and thermal triage.
- `docs/PERFORMANCE.md` outlines DeepStream batching, detector intervals, and `nvpmodel` presets.
- `docs/placeholders.md` serves as the live index for required secrets and config keys.

## Automation Agent Workflow
The maintainer bot runs via `make agent:refresh` or the scheduled workflow under `.github/workflows/agent-refresh.yml`. It:
- Scans for new vision models and datasets (`src/agent/webscan`).
- Ranks candidates based on deployment constraints (`src/agent/rank`).
- Updates documentation blocks and logs actions to `HANDOFF.md`.

Gate the agent with a dry run before allowing it to write:
```bash
make agent:refresh -- --dry-run
```
When satisfied, drop the `--dry-run` flag to persist updates.

## Security & Placeholder Hygiene
- Never substitute real credentials directly in committed files—leave `{{PLACEHOLDER}}` tokens intact.
- Use `make placeholders:check` to ensure all placeholders resolve before shipping production builds.
- Secrets live in `.env` (gitignored) and environment-specific secret stores.
- See [`docs/placeholders.md`](docs/placeholders.md) for the live inventory and resolution status.

## Governance & Next Steps
- Follow Conventional Commits and keep subject lines ≤72 characters.
- CI (see `.github/workflows/agent-ci.yml`) runs lint, tests, placeholder checks, and secret scanning.
- Record any roadmap items in [`docs/next_steps.md`](docs/next_steps.md) and [`docs/IMPLEMENTATION_STATUS.md`](docs/IMPLEMENTATION_STATUS.md) rather than inline TODO comments.
- The next structural milestone is migrating runtime modules under `src/aixavier/` per [`AGENTS.md`](AGENTS.md).

## Live Auto Sections
<!-- auto:start name=recommended-models -->
| Rank | Task                 | Model                 | FP16 (ms) | INT8 (ms) | Accuracy | License    | Last Checked |
| ---- | -------------------- | --------------------- | --------- | --------- | -------- | ---------- | ------------ |
| 1    | face_recognition     | MobileFaceNet ArcFace | 2.10      | 1.50      | 0.996    | MIT        | 2025-09-27   |
| 2    | reid                 | OSNet_x0_25           | 1.90      | 1.30      | 0.508    | MIT        | 2025-09-22   |
| 3    | face_detection       | SCRFD-500M            | 6.20      | 4.10      | 0.915    | MIT        | 2025-09-27   |
| 4    | fire_smoke_detection | EfficientNet-B0 Fire  | 9.80      | 6.70      | 0.923    | Apache-2.0 | 2025-09-18   |
| 5    | violence_detection   | MobileNet-TSM         | 12.90     | 9.00      | 0.842    | Apache-2.0 | 2025-09-21   |
| 6    | pose_estimation      | yolov11n-pose         | 11.30     | 7.90      | 0.608    | GPLv3      | 2025-09-30   |
<!-- auto:end -->

<!-- auto:start name=performance-tuning -->
- **DeepStream primary-gie batching**: Use batch-size=2 for yolov11n on AGX when PROFILE=all; falls back to 1 on NX. (_Impact_: Improves ingest FPS by ~8% while keeping latency under 40 ms.)
- **TensorRT DLA offload**: Assign SCRFD embedding head to DLA0 in demo profile to free SMs for violence detector. (_Impact_: Reduces GPU util by 6% during peak loads.)
- **Recorder I/O buffering**: Set GStreamer queue `max-size-time=3000000000` (3 s) for circular buffer stability. (_Impact_: Prevents underflow when writing to slower eMMC storage.)
<!-- auto:end -->

<!-- auto:start name=known-issues -->
- **[MEDIUM] INT8 export fails for YOLOv10n on TensorRT 8.6.1** — The TensorRT ONNX parser in JetPack 5.1.2 lacks support for new YOLOv10 detection head ops. _Impact_: INT8 acceleration unavailable for YOLOv10n until JetPack 6 or custom plugin compiled. _Workaround_: Stick to FP16 or backport the opset-18 plugin from TensorRT OSS. [Details](https://github.com/THU-MIG/yolov10/issues/42)
- **[LOW] SCRFD false positives in dense steam scenarios** — Steam near coach doors triggered high-confidence face detections. _Impact_: FRS blur triggers and audit noise during cleaning cycles. _Workaround_: Enable ROI mask for door corners and tighten `FRS_THRESHOLD` to 0.52 when cleaning profile active. [Details](https://insightface.ai/docs/scrfd/troubleshooting)
- **[HIGH] X3D-S memory spike on Jetson Xavier NX 8GB** — Batch=2 clip inference spikes VRAM over 7.2GB causing OOM with UI co-located. _Impact_: Violence detector crashes on NX unless intervals increased. _Workaround_: Set `ACTION_CLIP_BATCH=1` and raise clip stride to 3. [Details](https://forums.developer.nvidia.com/t/x3d-s-oom/298741)
<!-- auto:end -->

<!-- auto:start name=changelog -->
- 2025-10-07: Catalog refresh (models=10, datasets=44, placeholders=40).
- 2025-10-07: Catalog refresh (models=10, datasets=44, placeholders=42).
- 2025-10-07: Initial repository scaffold for Jetson Xavier railway CCTV analytics.
- 2025-10-07: Updated README to document bootstrap commands, module layout, and upcoming `src/aixavier/` migration.
<!-- auto:end -->
- Detector post-processing unit tests rely on NumPy; set `AIXAVIER_ENABLE_NUMPY_TESTS=1` when that runtime is available.
# Repository Guidelines

This repository is being scaffolded for a collection of automation agents and supporting orchestration utilities. Follow the practices below so that early contributions converge toward a consistent, maintainable layout.

## Project Structure & Module Organization
- Keep runtime code in `src/aixavier/`, grouping agents under `src/aixavier/agents/` and shared abstractions under `src/aixavier/core/`.
- Detector runtimes temporarily live under `src/runners/` (`detectors.py` hosts the shared inference loop); plan to relocate to `src/aixavier/vision/` during the refactor.
- Place evaluation data, prompt templates, and fixtures in `assets/` with subfolders such as `assets/prompts/` and `assets/datasets/`.
- Store documentation artifacts in `docs/` and diagrams in `docs/diagrams/`.
- Co-locate smoke scripts or notebooks in `experiments/` but promote anything reusable into `src/`.
- House all automated tests in `tests/`, mirroring the `src/` package layout (e.g., `tests/agents/test_memory.py` exercises `src/aixavier/agents/memory.py`).

## Build, Test, and Development Commands
```bash
python -m venv .venv && source .venv/bin/activate      # bootstrap local environment
pip install -r requirements.txt                        # install core runtime deps
pip install -r requirements-dev.txt                    # install lint/test tooling
pytest                                                  # run the full unit and integration suite
ruff check src tests                                    # lint and static analysis
python -m build                                         # produce a distributable wheel/sdist
```

## Coding Style & Naming Conventions
- Use 4-space indentation, type hints, and Python 3.11+ features where they clarify intent.
- Run `black` (line length 100) and `ruff` before opening a pull request; configure editors to respect `.editorconfig`.
- Modules follow snake_case (`memory_router.py`), classes use PascalCase (`MemoryRouter`), and functions/variables use snake_case (`route_message`).
- Environment variables should be upper-case with underscores (`AGENT_TIMEOUT_SECONDS`) and defined in `.env.example`.
- When wiring detectors, ensure ONNX exports are staged under `models/<modality>/onnx/` for CPU validation and TensorRT engines under `models/usecases/<use-case>/` before enabling CI.
- Tracker utilities live in `src/trackers/bytetrack.py`; extend `SimpleTracker` or swap to full ByteTrack/ReID before productionization.

## Testing Guidelines
- Prefer `pytest` test modules named `test_<subject>.py`; within each module, describe behaviors (`def test_memory_router_retries_on_timeout():`).
- Add integration tests under `tests/integration/` when exercising external services or orchestrations.
- Target ≥90% coverage on critical agent pathways; update `tests/README.md` with edge cases when introducing new tooling.
- Use `pytest -k "<keyword>"` locally for focused runs and ensure CI passes before requesting review.
- Detector post-process tests require NumPy; export `AIXAVIER_ENABLE_NUMPY_TESTS=1` when the runtime is available.

## Commit & Pull Request Guidelines
- Adopt Conventional Commits from the outset (`feat: add planner agent skeleton`, `fix: tighten retry budget`); keep subject lines ≤72 characters.
- Squash work-in-progress commits locally; each PR should read as a cohesive story with linked issues or roadmap items.
- Pull requests must include: scope summary, testing notes (commands + results), screenshots or logs for UI/UX changes, and follow-up tasks if applicable.
- Tag reviewers responsible for the touched subsystem (`@agents`, `@infra`) and add TODOs to `docs/roadmap.md` instead of burying them in code comments.

## Security & Configuration Tips
- Never commit API keys or secrets; rely on `.env` (gitignored) and document required keys in `.env.example`.
- Rotate credentials quarterly and reference secret names (not values) in documentation.
- When integrating third-party tools, sandbox them under dedicated service accounts and capture onboarding steps in `docs/integrations/`.
version: "3.9"

x-common-env: &common-env
  PROFILE: ${PROFILE:-demo}
  PLACEHOLDER_RESOLVED: ${PLACEHOLDER_RESOLVED:-false}
  PROMETHEUS_SCRAPE_PORT: ${PROMETHEUS_SCRAPE_PORT:-9100}
  MQTT_BROKER_URL: ${MQTT_BROKER_URL:-mqtt://localhost:1883}
  STORAGE_CLIPS_PATH: ${STORAGE_CLIPS_PATH:-/data/recordings}
  STORAGE_EXPORT_PATH: ${STORAGE_EXPORT_PATH:-/data/exports}

x-common-volumes: &common-volumes
  - recordings:/data/recordings
  - exports:/data/exports
  - logs:/var/log/edge-cctv
  - ./configs:/app/configs:ro
  - ./docs:/app/docs:ro
  - ./tests/data:/app/tests/data:ro
  - ./models:/app/models:ro

services:
  ingest:
    build:
      context: .
      dockerfile: src/ingest_gst/Dockerfile
    runtime: nvidia
    environment:
      <<: *common-env
      CAMERA_CONFIG: /app/configs/cameras.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - exporter
    healthcheck:
      test: ["CMD", "python", "-m", "ingest_gst.healthcheck"]
      interval: 30s
      timeout: 5s
      retries: 3

  detect_object:
    build:
      context: .
      dockerfile: src/runners/Dockerfile.object
    runtime: nvidia
    environment:
      <<: *common-env
      DETECTOR_CONFIG: /app/configs/detectors/object.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - ingest

  detect_face:
    build:
      context: .
      dockerfile: src/runners/Dockerfile.face
    runtime: nvidia
    environment:
      <<: *common-env
      DETECTOR_CONFIG: /app/configs/detectors/face.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - ingest

  detect_action:
    build:
      context: .
      dockerfile: src/runners/Dockerfile.action
    runtime: nvidia
    environment:
      <<: *common-env
      DETECTOR_CONFIG: /app/configs/detectors/action.yaml
    volumes: *common-volumes
    profiles: [all]
    depends_on:
      - ingest

  detect_pose:
    build:
      context: .
      dockerfile: src/runners/Dockerfile.pose
    runtime: nvidia
    environment:
      <<: *common-env
      DETECTOR_CONFIG: /app/configs/detectors/pose_velocity.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - ingest

  tracker:
    build:
      context: .
      dockerfile: src/trackers/Dockerfile
    runtime: nvidia
    environment:
      <<: *common-env
      TRACKER_CONFIG: /app/configs/tracker.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - detect_object
      - detect_pose

  rules:
    build:
      context: .
      dockerfile: src/rules/Dockerfile
    environment:
      <<: *common-env
      RULES_DIR: /app/configs/usecases
      PROFILE_CONFIG: /app/configs/profile_${PROFILE:-demo}.yaml
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - tracker
      - privacy

  events:
    build:
      context: .
      dockerfile: src/events/Dockerfile
    environment:
      <<: *common-env
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - rules

  privacy:
    build:
      context: .
      dockerfile: src/privacy/Dockerfile
    runtime: nvidia
    environment:
      <<: *common-env
      FRS_THRESHOLD: ${FRS_THRESHOLD:-0.47}
    volumes:
      - embeddings:/app/data/embeddings
      - audits:/app/data/audits
      - ./configs/frs:/app/configs/frs:ro
      - ./docs:/app/docs:ro
    profiles: [all, demo]

  recorder:
    build:
      context: .
      dockerfile: src/recorder/Dockerfile
    environment:
      <<: *common-env
    volumes: *common-volumes
    profiles: [all, demo]
    depends_on:
      - ingest

  ui:
    build:
      context: .
      dockerfile: src/ui/Dockerfile
    environment:
      <<: *common-env
      UI_HOST: ${UI_HOST:-0.0.0.0}
      UI_PORT: ${UI_PORT:-8080}
    volumes: *common-volumes
    profiles: [all, demo]
    ports:
      - "${UI_PORT:-8080}:8080"
    depends_on:
      - events
      - privacy
      - recorder

  exporter:
    build:
      context: .
      dockerfile: src/exporter/Dockerfile
    environment:
      <<: *common-env
    volumes: *common-volumes
    profiles: [all, demo]
    ports:
      - "${PROMETHEUS_SCRAPE_PORT:-9100}:9100"

  agent:
    build:
      context: .
      dockerfile: src/agent/Dockerfile
    environment:
      <<: *common-env
      GIT_AUTHOR_NAME: agent
      GIT_AUTHOR_EMAIL: agent@example.com
    profiles: [all]
    depends_on:
      - exporter

  demo_rtsp:
    build:
      context: .
      dockerfile: tests/data/Dockerfile
    environment:
      DEMO_SOURCE: /data/demo_stream.mp4
    volumes:
      - ./tests/data:/data
    profiles: [demo]

volumes:
  recordings:
  exports:
  logs:
  embeddings:
  audits:
SHELL := /bin/bash
PYTHON := python3
VENV := .venv
ACTIVATE := source $(VENV)/bin/activate
PROFILE ?= demo
ARGS ?= --dry-run
CONFIG ?= configs/detectors/pose_velocity.yaml
SRC ?= webcam:0
OUTPUT ?= artifacts/live_demo
CAMERA ?= CAM_WEB
FPS ?= 25
SAVE_FRAMES ?= 0
LIVE_SAVE_FLAG := $(if $(filter 1 true TRUE yes YES,$(SAVE_FRAMES)),--save-frames,)
SHOW ?= 0
RECORD ?=
LIVE_SHOW_FLAG := $(if $(filter 1 true TRUE yes YES,$(SHOW)),--show,)
LIVE_RECORD_FLAG := $(if $(RECORD),--record $(RECORD),)

.DEFAULT_GOAL := help

help:
	@echo "Targets:" 
	@grep -E '^[a-zA-Z_-]+:.*?#' Makefile | sed 's/:.*?#/ - /'

bootstrap: requirements.txt requirements-dev.txt
	@test -d $(VENV) || $(PYTHON) -m venv $(VENV)
	$(ACTIVATE) && pip install --upgrade pip wheel
	$(ACTIVATE) && pip install -r requirements.txt -r requirements-dev.txt
	$(ACTIVATE) && python models/bootstrap_models.py --profile=$(PROFILE)
	@echo "Bootstrap complete."

run:
	PROFILE=$(PROFILE) docker compose --profile all up --build

run-detached:
	PROFILE=$(PROFILE) docker compose --profile all up --build -d

stop:
	docker compose down

demo:
	$(PYTHON) tests/data/generate_demo.py
	CAMERA_RTSP_URL_01=demo://synthetic PROFILE=demo docker compose --profile demo up --build

perf:
	$(ACTIVATE) && $(PYTHON) tests/test_perf.py --endpoint http://localhost:9100/metrics

live:
	$(ACTIVATE) && $(PYTHON) -m src.apps.live_demo --config $(CONFIG) --source $(SRC) --output $(OUTPUT) --camera-id $(CAMERA) --fps $(FPS) $(LIVE_SAVE_FLAG) $(LIVE_SHOW_FLAG) $(LIVE_RECORD_FLAG)

clean:
	docker compose down -v || true
	rm -rf $(VENV) artifacts/ logs/ cache/

lint:
	$(ACTIVATE) && ruff check src tools tests
	$(ACTIVATE) && mypy src tools

test:
	$(ACTIVATE) && pytest -q
	bash tests/test_rtsp_connect.sh

placeholders\:list:
	$(ACTIVATE) && $(PYTHON) tools/resolve_placeholders.py --list

placeholders\:resolve:
	$(ACTIVATE) && $(PYTHON) tools/resolve_placeholders.py --from $${FROM}

placeholders\:check:
	$(ACTIVATE) && $(PYTHON) tools/placeholder_lint.py

agent\:refresh:
	$(ACTIVATE) && $(PYTHON) src/agent/main.py $(ARGS)

format:
	$(ACTIVATE) && ruff check --fix src tools tests
	$(ACTIVATE) && black src tools tests

ci: lint test placeholders\:check

.PHONY: help bootstrap run run-detached stop demo perf live clean lint test ci format placeholders\:list placeholders\:resolve placeholders\:check agent\:refresh
configs:
cameras.yaml
detectors/
frs/
profile_all.yaml
profile_demo.yaml
profile_minimal.yaml
telemetry.demo.yaml
tracker.yaml
usecases/

configs/detectors:
action.yaml
face.yaml
fire_smoke.yaml
gesture.yaml
object.yaml
pose_velocity.yaml

configs/frs:
blacklist.yaml
config.yaml
roles.yaml
staff_whitelist.yaml
whitelist.yaml

configs/usecases:
calling_out_signal_aspect.yaml
camera_tampering.yaml
child_safety_monitoring.yaml
cleaning_hygiene_monitoring.yaml
coach_door_panel_security.yaml
emergency_brake_valve_spad.yaml
face_recognition.yaml
hand_gesture_signal_identification.demo.yaml
hand_gesture_signal_identification.yaml
human_aggression_violence.yaml
medical_emergency_collapse.demo.yaml
medical_emergency_collapse.yaml
mobile_phone_usage_detection.demo.yaml
mobile_phone_usage_detection.yaml
object_identification_classification.yaml
on_demand_viewing.yaml
panic_alarm_validation.yaml
passenger_falling_from_door.yaml
people_counting_occupancy.yaml
smoke_fire_haze.yaml
staff_movement_tracking.yaml
stone_pelting.yaml
trespassing_on_track.yaml
unattended_baggage.yaml
vandalism_detection.yaml

src:
__init__.py
__pycache__/
agent/
aixavier/
apps/
common/
events/
exporter/
ingest_gst/
models/
privacy/
recorder/
rules/
runners/
telemetry/
trackers/
ui/

src/__pycache__:
__init__.cpython-312.pyc

src/agent:
__init__.py
bench/
data/
Dockerfile
main.py
rank/
webscan/
writer/

src/agent/bench:
__init__.py

src/agent/data:
catalog.json

src/agent/rank:
__init__.py
__pycache__/
rank.py

src/agent/rank/__pycache__:
__init__.cpython-312.pyc
rank.cpython-312.pyc

src/agent/webscan:
__init__.py
__pycache__/
scan.py

src/agent/webscan/__pycache__:
__init__.cpython-312.pyc
scan.cpython-312.pyc

src/agent/writer:
__init__.py
__pycache__/
apply.py

src/agent/writer/__pycache__:
__init__.cpython-312.pyc
apply.cpython-312.pyc

src/aixavier:
__init__.py
__pycache__/
core/
ui/

src/aixavier/__pycache__:
__init__.cpython-312.pyc

src/aixavier/core:
__init__.py
__pycache__/
usecases.py

src/aixavier/core/__pycache__:
__init__.cpython-312.pyc
usecases.cpython-312.pyc

src/aixavier/ui:
__init__.py
api.py
event_browser.py
frontend_stub.py
README.md
roi_editor.py
toggles.py

src/apps:
__init__.py
__pycache__/
demo_runner.py
live_demo.py

src/apps/__pycache__:
__init__.cpython-312.pyc
live_demo.cpython-312.pyc

src/common:
__init__.py
__pycache__/
config.py
event_bus.py

src/common/__pycache__:
__init__.cpython-312.pyc
config.cpython-312.pyc
event_bus.cpython-312.pyc

src/events:
__init__.py
Dockerfile
main.py

src/exporter:
__init__.py
__pycache__/
Dockerfile
main.py

src/exporter/__pycache__:
__init__.cpython-312.pyc
main.cpython-312.pyc

src/ingest_gst:
__init__.py
__pycache__/
Dockerfile
healthcheck.py
main.py
pipeline.py

src/ingest_gst/__pycache__:
__init__.cpython-312.pyc
healthcheck.cpython-312.pyc
main.cpython-312.pyc
pipeline.cpython-312.pyc

src/models:
__init__.py
__pycache__/
collapse.py
gesture.py
phone.py

src/models/__pycache__:
__init__.cpython-312.pyc
collapse.cpython-312.pyc
gesture.cpython-312.pyc
phone.cpython-312.pyc

src/privacy:
__init__.py
Dockerfile
main.py
store.py

src/recorder:
__init__.py
__pycache__/
circular_buffer.py
Dockerfile
main.py

src/recorder/__pycache__:
__init__.cpython-312.pyc
circular_buffer.cpython-312.pyc

src/rules:
__init__.py
__pycache__/
Dockerfile
engine.py
main.py

src/rules/__pycache__:
__init__.cpython-312.pyc
engine.cpython-312.pyc

src/runners:
__init__.py
__pycache__/
detectors.py
Dockerfile.action
Dockerfile.face
Dockerfile.object
Dockerfile.pose
main.py
pose_assoc.py
pose_events.py
postprocess.py
renderer.py

src/runners/__pycache__:
__init__.cpython-312.pyc
detectors.cpython-312.pyc
main.cpython-312.pyc
pose_assoc.cpython-312.pyc
pose_events.cpython-312.pyc
postprocess.cpython-312.pyc
renderer.cpython-312.pyc

src/telemetry:
__init__.py
__pycache__/
ingest.py

src/telemetry/__pycache__:
__init__.cpython-312.pyc
ingest.cpython-312.pyc

src/trackers:
__init__.py
__pycache__/
bytetrack.py
Dockerfile
main.py
osnet_reid.py

src/trackers/__pycache__:
__init__.cpython-312.pyc
bytetrack.cpython-312.pyc
osnet_reid.cpython-312.pyc

src/ui:
__init__.py
app.py
dashboard.py
Dockerfile
